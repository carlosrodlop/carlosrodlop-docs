# AWS Solution Architect

![badge](https://images.credly.com/size/340x340/images/0e284c3f-5164-4b21-8660-0d84737941bc/image.png)

## Index

1. [Global](#global)
2. [Security](#security)
3. [Compute](#compute)
4. [Application Integration](#application_integration)
5. [Storage](#storage)
6. [Database](#database)
7. [Migration](#migration)
8. [Networking](#networking)
9. [Management_and_Governance](#management_and_governance)
10. [Containers](#containers)
11. [References](#references)

## Global

Go to [Index](#index)

### Infrastructure

- [30 Regions with 96 Availability Zones](http://clusterfrak.com/notes/certs/aws_saa_notes/#security-and-identity)
- Edge locations are CDN endpoints for CloudFront. Currently there are over 50 edge locations. Not the same as an AZ
- Maximum Response time for Business is 1 hour
- Services such as CloudFormation, Elastic Beanstalk, Autoscaling, and OpsWorks are free however resources generated by these services are not free

#### AWS Region

- AWS regions are physical locations around the world having cluster of data centers
- You need to select the region first for most of the AWS services such as EC2, ELB, S3, Lambda, etc.
- You can not select region for Global AWS services such as IAM, AWS Organizations, Route 53, CloudFront, WAF, etc.
- Each AWS Region consists of multiple, isolated, and physically separate AZs (Availability Zones) within a geographic area.

#### AWS AZ (Availability zones)

- An AZ is one or more discrete data centers with redundant power, networking, and connectivity
- All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking.
- Customer deploy applications across multiple AZs in same region for high-availability, scalability, fault-tolerant and low-latency.
- AZs in a region are usually 3, min is 2 and max is 6 for e.g. 3 AZs in Ohio are us-east-2a, us-east-2b, and us-east-2c.
- For high availability in us-east-2 region with min 6 instances required either place 3 instances in each 3 AZs or place 6 instances in each 2 AZs (choose any 2 AZs out of 3) so that it works normal when 1 AZ goes down.

### Tags

- Key/Value pairs attached to AWS resources
- Metadata (data about data).
- Sometimes can be inherited (Auto-scaling, CloudFormation, Elastic Beanstalk can create other resources)
- Resource Groups make it easy to group your resources using the tags that are assigned to them
  - You can group resources that share one or more tags
  - Resource groups contain info such as region, name, health checks

### Amazon Resource Name (ARN)

- Identifier (ID) for any AWS resource. They are globally unique
- begin with `arn:<partition>:<service>:<region>:<account_id>` and end with: `resource`
  `resource_type/resource`, `resource_type/resource/qualifier`, `resource_type/resource:qualifier`, `resource_type:resource` and `resource_type:resource:qualifier`
- Examples:
  - `arn:aws:iam::123456789012:user/sri` <-- :: region is omitted because IAM is global
  - `arn:aws:s3:::my_awesome_bucket/image.png` <-- ::: - no region, no account id needed to identify an object in S3. all objects in S3 are globally unique
  - `arn:aws:dynamodb:us-east-1:123456789012:table/orders`
  - `arn:aws:ec2:us-east-1:123456789012:instance/*` - wildcard. EC2 is a regional service

### Consolidated Billing

- Accounts roll for customers:
  - Paying account is independent, can not access resources of the other accounts
  - Linked accounts are independent from one another
  - Currently there is a limit of 20 linked accounts for consolidated billing (soft limit)
  - One bill per AWS account
  - Easy to track charges and allocate costs between linked accounts
  - Volume pricing discount
  - Resources across all linked accounts are tallied, and billing is applied collectively to allow bigger discounts

### Best Practices

- Business Benefits of Cloud:

  - Almost 0 upfront infrastructure investment
  - Just in time Infrastructure
  - More efficient resource utilization
  - Usage based costing
  - Reduced time to market

- Technical Benefits of Cloud:

  - Automation - Scriptable infrastructure
  - Auto-Scaling
  - Proactive Scaling
  - More efficient development life cycle
  - Improved testability
  - DR and Business Continuity
  - Overflow the traffic to the cloud

- Design for Failure

  - Rule of thumb: Be a pessimist when designing architectures in the cloud
  - Assume things will fail, always design implement and deploy for automated recovery from failure
  - Assume your hardware will fail
  - Assume outages will occur
  - Assume that some disaster will strike your application
  - Assume that you will be slammed with more than the expected number of requests per second
  - Assume that with time your application software will fail too

- Decouple your components:
  - Think SQS
  - Build components that do not have tight dependencies on each other so that if one component dies, fails, sleeps, or becomes busy, the other components are built so they can continue to work as if no failure is happening. Build each component as a black box

### [Service Quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) (known before as Service Limits)

- Your AWS account has default quotas, formerly referred to as limits, for each AWS service. Unless otherwise noted, each quota is Region-specific. You can request increases for some quotas, and other quotas cannot be increased.

### Types of Cloud Computing

#### Services Models

- Infrastructure as a Service - **IaaS** (Examples: EC2, ELB, VPC)
  
  - Basic building blocks for cloud IT and typically provide access to networking features, computers (virtual or on dedicated hardware), and data storage space.
  - Highest level of flexibility and management control over your IT resources

- Platform as a Service - **PaaS** (Examples: Elastic Beanstalk, Fargate)

  - Platforms as a service remove the need for organizations to manage the underlying infrastructure (usually hardware and operating systems)
  - This helps you be more efficient as you don’t need to worry about resource procurement, capacity planning, software maintenance, patching, or any of the other undifferentiated heavy lifting involved in running your application.

- Software as a Service - **SaaS** (Examples: ECS, Aurora, ECR)

  - It is completed product that is run and managed by the service provider ("end-user applications").
  - You do not not only need to think about how the underlying infrastructure is managed (PaaS) but also how the service is maintained; you only need to think about how you will use that particular piece of software

#### Deployment Models

- Cloud

  - A cloud-based application is fully deployed in the cloud and all parts of the application run in the cloud.

- [Hybrid](https://aws.amazon.com/hybrid/)

  - A hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud.
  - Use: Between cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to internal system.

- On-Premises (Private)

  - Deploying resources on-premises, using virtualization and resource management tools, is sometimes called “private cloud”.
  - It does not provide many of the benefits of cloud computing.

## Security

Go to [Index](#index)

### IAM (Global Service)

- AWS Identity and Access Management Service (AWS IAM) is used to securely control **individual and group access to AWS resources**.

![IAM](https://d1.awsstatic.com/product-marketing/IAM/iam-how-it-works-diagram.04a2c4e4a1e8848155840676fa97ff2146d19012.png)

- IAM is global (so region isn’t a factor)
- It offers centralized control over your AWS account. It enables shared access to your AWS account.
- Granular Permissions (can set different permissions for different people/ different resources)
- Includes Federation Integration which taps into Active Directory, Facebook, Linkedin, etc. for authentication
- Multi-factor authentication support
- Allows configuration of temporary access for users, devices and services
- It supports PCI DSS compliance. PCI DSS compliance just is basically a compliant framework that if you're taking credit card details, you need to be compliant with the framework. So IAM supports PCI DSS.
- Access entities:
  - `Users` End users such as people, employees of an organisation ... etc
    - IAM users are individuals who have been granted access to an AWS account.
    - New Users
      - They have NO permissions when first created, NO have access to any AWS services (they can only login to the AWS console) ==> Permission must be explicitly granted to allow a user to access an AWS service.
      - They are assigned Access Key ID & Secret Access Keys when first created => You can get to view these once. If you lose them, you have to regenerate them.
      - In order for a new IAM user to be able to log into the console, the user must have a password set
  - `Groups` are a collection of users, and can not have other groups. Groups allow you to define permissions for all the users within it.
  - `Roles`
    - Types of Roles
      - Service Roles (To assign permission to AWS Resources), specifying what the resource (such as EC2) is allowed to access on another resource (S3)
      - Cross account access roles: Used when you have multiple AWS accounts and another AWS account must interact with the current AWS account
      - Identity provider access roles : Identity Federation (including AD, Facebook etc) can be configured to allow secure access to resources in an AWS account without creating an IAM user account.
    - Adventanges
      - Roles are more secure than storing your access key and secret access key on individual EC2 instances.
      - Roles are easier to manage.
      - Roles can be assigned to an EC2 instance after it is created using both the console & command line.
- `Policies` JSON Document that defines permissions (`Allow` or `Deny` access to an action that can be performed on AWS resources) for Access Entities (user, group or role)
  - Each statement matches an AWS API request
  - Anything that is not explicitly allowed is implicitly denied
    - **IAM Policy Evaluation Logic** ➔ Explicit Deny ➯ Organization SCPs ➯ Resource-based Policies (optional) ➯ IAM Permission Boundaries ➯ Identity-based Policies
  - If a resource has multiple policies — AWS joins them
  - The **Least Privilege Principle** should be followed in AWS, don’t give more permission than a user needs.
  - **IAM Permission Boundaries** to set at individual user or role for maximum allowed permissions
  - **Resource Based Policies** are supported by S3, SNS, and SQS.
  - Sections:
    - `Version` policy language version. `2012-10-17` is the latest version.
    - `Statement` container for one or more policy statements
    - `Sid` (optional) a way of labeling your policy statement
    - `Effect` set whether the policy Allows or Deny
    - `Principal` user, group, role, or federated user to which you would like to allow or deny access
    - `Action` one or more actions that can be performed on AWS resources
    - `Resource` one or more AWS resources to which actions apply
    - `Condition` (optional) one or more conditions to satisfy for policy to be applicable, otherwise ignore the policy.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Deny-Barclay-S3-Access",
      "Effect": "Deny",
      "Principal": { "AWS": ["arn:aws:iam:123456789012:barclay"] },
      "Action": ["s3:GetObject", "s3:PutObject", "s3:List*"],
      "Resource": ["arn:aws:s3:::mybucket/*"]
    },
    {
      "Effect": "Allow",
      "Action": "iam:CreateServiceLinkedRole",
      "Resource": "*",
      "Condition": {
        "StringLike": {
          "iam:AWSServiceName": [
            "rds.amazonaws.com",
            "rds.application-autoscaling.amazonaws.com"
          ]
        }
      }
    }
  ]
}
```

- `Root account` is created by default with full administrator. Root account is email address that you used to register your account
  - Best practices
    - Not to use the root account for anything other than billing (not login)
    - Always setup Multifactor Authentication on your root account.
- `Power user access` → Access to all AWS services except the management of groups and users within IAM

### Access AWS

- Temporary security credentials/access consist of the AWS access key ID, secret access key, and `security token` (temporal/expire).
  - AWS Security Token Service (AWS STS) as a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users)
- Types of Access
  - IAM Users
  - None IAM Users

#### IAM Users

- Options:

  - AWS Management Console - Use password + MFA (multi factor authentication)
  - AWS CLI or SDK - Use Access Key ID (~username) and Secret Access Key (~password)
  - AWS CloudShell - CLI tool from AWS browser console - Require login to AWS

- When creating a user's credentials, you can only see/download the credentials at the time of creation not after.
- Access Keys can be retired, and new ones can be created in the event that secret access keys are lost
- To create an user password (loging in AWS console), once the users have been created,  you can opt to create a generated or custom password.
  - If generated, there is an option to force the user to set a custom password on next login.
  - Once a generated password has been issued, you can see the password which is the same as the access keys. Its shown once only.

#### None IAM Users

- Non-IAM user first authenticates from Identity Federation ==> Then provide a temporary token (IAM Role attached) generated by calling a AssumeRole API of `STS (Security Token Service)` ==> Non-IAM user access the AWS resource by **assuming IAM Role attached with token**.

##### Identity Federation

- SAML 2.0 (old) to integrate Active Directory/ADFS, use AssumeRoleWithSAML STS API
- Custom Identity Broker used when identity provider is not compatible to SAML 2.0, use AssumeRole or GetFederationToken STS API
- Web Identity Federation is used to sign in using well-known external identity provider (IdP), such as login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible IdP. Get the ID token from IdP, use AWS Cognito api to exchange ID token with cognito token, use AssumeRoleWithWebIdentity STS API to get temp security credential to access AWS resources
- AWS Cognito (`Amazon Cognito Federated Identities`) is recommended identity provider by Amazon
- Amazon Single Sign On gives single sign on token to access AWS, no need to call STS API

##### AWS Directory Service

- Directories store information about users, groups, and devices, and administrators use them to manage access to information and resources. **Hierarchical database of users, groups, computers - trees and forests**.

###### Compatible with Microsft Active Directory

- `Managed Microsoft Active Directory`

  - It is managed Microsoft Windows Server AD with trust connection to on-premise Microsoft AD.
  - Best choice when you need all AD features to support AWS applications or Windows workloads.
  - Easily migrate on-premise workloads as it is built on actual Microsoft AD, so does not require any replication of existing directory to the cloud.
  - Highly available as directories are deployed across multiple Availability Zones and failovers are detected automatically.
  - A common use case would be to extend your on-premise using AD Trust with AWS Managed Microsoft AD so that both your on-premises and cloud directories remain separated, but it allows your users access AWS as needed.

- `Simple AD`

  - Use Simple AD is standalone AWS managed compatible AD powered by Samba 4 with basic directory features (Enables a subset of the features Managed Microsoft AD)
  - You cannot connect it to on-premise AD.
  - Best choice for basic directory features.
  - Can be used for Linux workloads that need LDAP

- `AD Connector`
  - It is proxy service to redirect requests to on-premise Microsoft AD, without caching information in the cloud.
  - Best choice to use existing on-premise AD with compatible AWS services.
  - Can use multiple AD Connectors to spread the load to match performance needs.
  - Cannot be used across different AWS accounts.

###### No Compatible with Microsft Active Directory

- `Cloud Directory`

  - Cloud-native directories for organizing Hierarchies of data along **multiple dimensions** fully managed by AWS
  - Can have multiple hierarchies with hundreds/millions of objects.
  - Some common use cases include: directories for organisational charts, course catalogs, and device registries.

- `Amazon Cognito`
  - It control uses authentication (sign-up and sign-in) + access (premissions) for **mobile and web applications**. Supports guest users.
  - The two main components of Amazon Cognito are:

a. `User pools`: User directories in Amazon Cognito. Options for authetication

- _Directly through Amazon Cognito_
- Through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers.

In any case, members of the user pool have a directory profile that you can access through a Software Development Kit (SDK)

![Amazon Cognito, User pools](https://docs.aws.amazon.com/images/cognito/latest/developerguide/images/scenario-authentication-cup.png)

b. `Identity pools`: Cognito elements grant users temporary credentials to other AWS services (e.g., Amazon S3 and DynamoDB).

Steps: You first authenticate user using `Cognito User Pools` and then exchange token with `Cognito Identity Pools` which further use `AWS STS` to generate temporary AWS credentials to access AWS Resources.

### AWS Key Management Service (KMS)

![AWS Key Management Service (KMS)](https://d1.awsstatic.com/Security/aws-kms/Group%2017aws-kms.6dc3dbbbe5b75b46c4f62218d0531e5bed7276ce.png)

- AWS managed centralized key management service to create, manage and rotate customer master keys (CMKs) for encryption at REST. Provides you with a central place to manage all keys.
- Can integrate with most other AWS services to increase security and make it easier to encrypt your data.
- You can enable automatic master key rotation once per year. Service keeps the older version of master key to decrypt old encrypted data.
- Allows you to control access to the keys using things like IAM policies or key policies.
- Encrypt/decrypt up to 4KB.
- Pay per API call.
- Validated under FIPS 140–2 (**Level 2**) security standard.
- Types of Customer Master Keys (CMKs)
  - `Customer Managed CMKs` (Dedicated to my account) → Keys that you have created in AWS, that you own and manage. You are responsible for managing their key policies, rotating them and enabling/disabling them.
    - You can create customer-managed `Symmetric` (single key for both encrypt and decrypt operations) or `Asymmetric` (public/private key pair for encrypt/decrypt or sign/verify operations) master keys
    - Symmetric CMKs
      - With symmetric keys, the same key is used to encrypt and decrypt
      - The key never leaves AWS unencrypted
      - Must call the KMS API to use a symmetric key
      - The AWS services that integrate with KMS use symmetric CMKs
    - Asymmetric CMKs
      - Asymmetric keys are mathematically related public and private key pairs.
      - The private key never leaves AWS unencrypted.
      - You can call the KMS API with the public key, which can be downloaded and used outside of AWS.
      - AWS services that integrate with KMS DO NOT support asymmetric keys.
  - `AWS Managed CMKs` (Dedicated to my account) → These are free and are created by an AWS service on your behalf and are managed for you. However, only that service can use them. Used by default if you pick encryption in most AWS services
  - `AWS Owned CMKs` (No Dedicated to my account) → owned and managed by AWS and shared across many accounts.

### AWS CloudHSM

![AWS CloudHSM](https://d1.awsstatic.com/whiteboard-graphics/products/CloudHSM/product-page-diagram_AWS-CloudHSM_HIW.76ce14889e22d8861a6a9fff0b5664516ed1bddd.png)

- Dedicated cloud-based Hardware Security Module (HSM) for creating, using and managing your own encryption keys (cryptographic keys) in AWS.
- Conforms to FIPS 140–2 (**Level 3**) security standard
- No access to the AWS managed component and AWS does not have visibility or access to your keys.
- Integrate with your application using industry-standard APIs (No AWS APIs), such as PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG) libraries (there are no AWS APIs for HSM)
- CloudHSM runs within a VPC in your account
  - CloudHSM will operate inside its own VPC dedicated to CloudHSM
  - CloudHSM will project to ENI of customer VPC
- Keys are irretrievable if lost and can not be recovered.
- Use case: Use KMS to create a CMKs in a custom key store and store non-extractable key material in AWS CloudHSM to get a full control on encryption keys
- Difference between KMS and CloudHSM
  - FIPS 140–2 Level 2 vs Level 3
  - We manage our keys
    - KMS (Multi Tenant) and CloudHSM (Single Tenant, dedicate h/w, Mutli-AZ cluster)

### AWS Systems Manager

![AWS Systems Manager](https://d1.awsstatic.com/AWS%20Systems%20Manager/Product-Page-Diagram_AWS-Systems-Manager.9184df66edfbc48285d16c810c3f2d670e210479.png)

- `Parameter Store` is Secure and centralized serverless storage of configuration and secrets: passwords, database details, and license code, API Keys
  - `Parameter` value can be type String (plain text), StringList (comma separated) or SecureString (KMS encrypted data)
  - `Use case`: Centralized configuration for dev/uat/prod environment to be used by CLI, SDK, and Lambda function
- `Run Command` allows you to automate common administrative tasks and perform one-time configuration changes on EC2 instances at scale
- `Session Manager` replaces the need for Bastions to access instances in private subnet

### AWS Secrets Manager

![AWS Secrets Manager](https://d1.awsstatic.com/diagrams/Secrets-HIW.e84b6533ffb6bd688dad66cfca36622c2fa7c984.png)

- Secret Manager is mainly used to store, manage, and rotate secrets (passwords) such as database credentials, API keys, and OAuth tokens.
- Apply the new key/passwords in RDS for you. Generate random secrets.
- Secret Manager has **native support to rotate database credentials of RDS databases** - MySQL, PostgreSQL and Amazon Aurora. Automatically rotate secrets.
- For other secrets such as API keys or tokens, you need to use the **lambda for customized rotation function**.

### AWS Shield

![AWS Shield](https://d1.awsstatic.com/AWS%20Shield%402x.1d111b296bfd0dd864664b682217bc7610453808.png)

- AWS Shield provide protections against **Distributed Denial of Service (DDoS) attacks** for AWS resources at the network and transport layers (layer 3 and 4) and the application layer (layer 7)
  - Denial-of-service attack (DoS attack) is a cyber-attack in which the perpetrator seeks to make a machine or network resource unavailable to its intended users by temporarily or indefinitely disrupting services of a host connected to a network. Denial of service is typically accomplished by flooding the targeted machine or resource with superfluous requests in an attempt to overload systems and prevent some or all legitimate requests from being fulfilled
- Types
  - `AWS Shield Standard` is automatic and free DDoS protection service for all AWS customers for CloudFront and Route 53 resources.
  - `AWS Shield Advanced` is paid service (Advanced costs $3K per month per org) for enhanced DDoS protection for EC2, ELB, CloudFront, and Route 53 resources.

### AWS WAF

![AWS WAF](https://d1.awsstatic.com/Product-Page-Diagram_AWS-Web-Application-Firewall%402x.5f24d1b519ed1a88b7278c5d4cf7e4eeaf9b75cf.png)

- **Web Application Firewall** add an extra layer of protection to your web applications or APIs against web attacks from common exploits, such as SQL injection or Cross-site scripting (XSS).
  - SQL injection is a code injection technique used to attack data-driven applications, in which malicious SQL statements are inserted into an entry field for execution (e.g. to dump the database contents to the attacker).
  - Cross-site scripting (XSS) attacks enable attackers to inject client-side scripts into web pages viewed by other users. A cross-site scripting vulnerability may be used by attackers to bypass access controls such as the same-origin policy.
- You can deploy WAF on CloudFront, Application Load Balancer, API Gateway and AWS AppSync
- Protect against **Layer 7** (HTTP & HTTPS) attacks and block common attack patterns by setting up rules to control the traffic. How?
  - Setting up your own rules to control the traffic by either only allowing what you specify or only blocking what you specify. Alternatively, you can count the requests that match a certain pattern.
  - AWS also provides managed rules that you can use to get started quickly, these are fully pre-configured and cover things like the OWASP Top 10 Security risks.
  - Conditions are used in WAFs to specify when you want to allow/block requests. Below are some examples of conditions that you might:
    - Values on the request header
    - The country a request comes from
    - Specific IP addresses
    - Strings that appear in requests
    - Length of the request
    - Presence of SQL code
    - Presence of a script
- Pay for what you use, based on the number of rules you have and requests your applications receive.

### AWS Firewall Manager

![AWS Firewall Manager](https://d1.awsstatic.com/products/firewall-manager/product-page-diagram_AWS-Firewall-Manager%402x%20(1)1.ad6bf5281dc2c33c0493e9988e3504dd1590eaa2.png)

- Use AWS Firewall Manager to centrally configure and manage Firewall Rules across an Organization: AWS WAF rules, AWS Shield Advanced, Network Firewall rules, and Route 53 DNS Firewall Rules
- Use case: Meet Gov regulations to deploy AWS WAF rule to block traffic from embargoed countries across accounts and resources

### AWS GuardDuty

![AWS GuardDuty](https://d1.awsstatic.com/Security/Amazon-GuardDuty/Amazon-GuardDuty_HIW.057a144483974cb73ab5f3f87a50c7c79f6521fb.png)

- Read VPC Flow Logs, DNS Logs, and CloudTrail events. Apply machine learning algorithms and anomaly detections to discover threats
- Can protect against CryptoCurrency attacks

### Amazon Inspector

![Amazon Inspector](https://d1.awsstatic.com/reInvent/re21-pdp-tier1/amazon-inspector/Amazon-Inspector_HIW%402x.c26d455cb7e4e947c5cb2f9a5e0ab0238a445227.png)

- Automated Security Assessment service for **EC2 instances** by installing an agent in the OS of EC2 instance.
- Inspector comes with pre-defined rules packages:
  - `Network Reachability` rules package checks for unintended network accessibility of EC2 instances
  - `Host Assessment rules` package checks for vulnerabilities and insecure configurations on EC2 instance. Includes Common Vulnerabilities and Exposures (CVE), Center for Internet Security (CIS) Operating System configuration benchmarks, and security best practices.

### Amazon Macie

![Amazon Macie](https://d1.awsstatic.com/reInvent/reinvent-2022/macie/Product-Page-Diagram_Amazon-Macie.a51550cca0a731ba2e4a26e8463ed5f5a81202e3.png)

- Managed service to discover and protect your **sensitive data** in AWS
- Can automatically discover **Personally Identifiable Information (PII)** in your data and can alert you once identified (e.g. selected S3 buckets)
- Can produce dashboards, reporting and alerts
- Benefits of Macie
  - Cost efficiently discovers sensitive data at scale
  - Constant visibility of data security/privacy through alerts and dashboards
  - Can use used across AWS accounts through AWS Organisations.
  - Can help you meet regulatory compliance
  - Great for PCI-DSS(credit card payments) and preventing theft

### AWS Config

![AWS Config](https://d1.awsstatic.com/config-diagram-092122.974fe2a4cb6aae1fe564fdbbe30ab55841a9858e.png)

- Managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. Assess, audit, and evaluate configurations of your AWS resources in multi-region, multi-account
- You are notified via SNS for any configuration change
- Integrated with CloudTrail, provide resource configuration history
- Use case: Customers need to comply with standards like PCI-DSS (Payment Card Industry Data Security Standard) or HIPAA (U.S. Health Insurance Portability and Accountability Act) can use this service to assess compliance of AWS infra configurations

## Compute

Go to [Index](#index)

### EC2 (IaaS)

- Infrastructure as a Service (IaaS) - Re-sizable (elastic) and secure virtual machine on the cloud.
  - Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change.
  - Gives you complete control of your computing resources including choice of storage, processor, networking and operating system.
- When you restart an EC2 instance, its public IP can change. Use `Elastic IP` to assign a fixed public IPv4 to your EC2 instance. By default, all AWS accounts are limited to five (5) Elastic IP addresses per Region.
- The EC2 Root volume is a virtual disk where the OS is installed, it can only be launched on SSD or Magnetic.
- Bootstrap scripts are code that gets ran as soon as your EC2 instance first boots up.
- EC2 Information Endpoints (can be obteined via `curl`):
  - `http://169.254.169.254/latest/meta-data` ==> Metadata Private & public IP
  - `http://169.254.169.254/latest/user-data` ==> user-defined data
- Use VM Import/Export to import virtual machine image and convert to Amazon EC2 AMI to launch EC2 instances
- Termination protection is turned off by default.
- On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated. Any additional EBS volumes by default won't be deleted.
- Encryption
  - You need to create a key pair — public & private for asymmetric encryption.
  - Root device volumes can be encrypted now (a popular exam topic)
  - EBS Root volumes of your DEFAULT AMI's CAN be encrypted. You can also use a third party tool (such as bit locker etc) to encrypt the root volume, or this can be done when creating AMI's in the AWS console or using the API.
  - Additional volumes can be encrypted as well.
- You must provision nitro-based EC2 instance to achieve 64000 EBS IOPS. Max 32000 EBS IOPS with Non-Nitro EC2.
- EC2 Design
  - Place all the EC2 instances in same AZ to reduce the data transfer cost.
  - Design for failure. Have one EC2 instance in each availability zone.

#### EC2 Hibernate

- Allows you to hibernate your EC2 instances, so that you can stop them and pick back up where you left off again.
- It does this by saving the content from the in-memory state of the instance (RAM) to your EBS root volume.
- You can hibernate an instance only if it’s enabled for hibernation and it meets the hibernation prerequisites.
- Useful for long running services and services that take long to boot.
- Can’t hibernate for more than 60 days.
- Once in hibernation mode there is no hourly charge — you only pay for the elastic IP Address & other attached volumes.
- Boots up a lot faster after hibernation as it does not need to reload the operating system.

#### EC2 Instance Types

You can choose EC2 instance type based on requirement for e.g. `m5.2xlarge` has Linux OS, 8 vCPU, 32GB RAM, EBS-Only Storage, Up to 10 Gbps Network bandwidth, Up to 4,750 Mbps IO Operations.

| Instance Class | Usage Type            | Usage Example                                                                                                            |
| -------------- | --------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| T, M           | General Purpose       | Web Server, Code Repo, Microservice, Small Database, Virtual Desktop, Dev Environment                                    |
| C              | Compute Optimized     | High Performance Computing (HPC), Batch Processing, Gaming Server, Scientific Modelling, CPU-based machine learning      |
| R, X, Z        | Memory Optimized      | In-memory Cache, High Performance Database, Real-time big data analytics                                                 |
| F, G, P        | Accelerated Computing | High GPU, Graphics Intensive Applications, Machine Learning, Speech Recognition                                          |
| D, H, I        | Storage Optimized     | EC2 Instance Storage, High I/O Performance, HDFS, MapReduce File Systems, Spark, Hadoop, Redshift, Kafka, Elastic Search |

#### EC2 Pricing Types

- **On-Demand** - Pay a fixed rate by the hour (or by the second) with no commitment. Pay as you use, costly.
  - Use Cases:
    - Users that want the low cost and flexibility of Amazon EC2 without any up-front payment for long-term commitment.
    - Applications with short term, spiky or unpredictable workloads that cannot be interrupted.
    - Applications being developed or tested on Amazon EC2 for the first time.
- **Reserved** - Provides you with a capacity reservation, and offer significant discount on the hourly charge for an instance, but it requires to have a Contracts for 1 - 3 year terms. Higher discount with upfront payments and longer contracts. However, you can't move between regions.
  - Uses Cases:
    - Applications with steady or predictable usage.
    - Applications that require reserved capacity.
    - Users able to make upfront payments to reduce their total computing costs even further.
  - Types:
    - **Standard Reserved Instances** Provides the most discount (up to 75% off). Unused instanced can be sold in AWS reserved instance marketplace
    - **Convertible Reserved Instances** up to 54% off. It can be exchanged for another Convertible Reserved Instance with different instance attributes e.g. you to change between instance types e.g. t1-t4 as long as its of greater or equal value
    - **Scheduled Reserved Instances** - reserve capacity that is scheduled to recur daily, weekly, or monthly, with a specified start time and duration, for a one-year term.
- **Spot Instances** - Enables you to bid whatever price you want for instance capacity. when AWS has excess capacity it drops the price so people can use that capacity —but they can take it back at any time. You can set the price you are willing to pay and it will run when its below or at that price — if it goes above that price you lose it.
  - It provides up to 90% discount and typically used for apps with flexible start/end times, But don’t use for anything critical that needs to be online all the time. It can handle interruptions and recover gracefully.
  - Imp Note: If the spot instance is terminated by Amazon EC2, you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged for any hour in which the instance ran.
  - Uses Cases
    - Applications that have flexible start and end times.
    - Applications that are only feasible at very low compute prices.
    - Users with urgent computing needs for large amounts of additional capacity.
  - Types
    - **Spot Blocks** can also be launched with a required duration, which are not interrupted due to changes in the Spot price.
    - **Spot Fleet**
      - Collection of spot instances and optionally on-demand instances. Attempts to launch a number of them together to meet a certain capacity within your price budget.
      - The allocation of spot instances depends on how they fulfil your spot fleet request from the possible pool of instances.
      - Strategies:
        - Lowest Price → This is the default strategy. Chooses the fleet pool with the lowest price.
        - Diversified → Distributed across all pools.
        - Capacity Optimised → Pool for optimal capacity for the number of instances launching.
        - InstancePoolsToUseCount → Distributed across the number of pools you specify — this can only be used with the lowest price option.

- **Dedicated Instance** - Your instance runs on a dedicated hardware provide physical isolation, single-tenant
- **Dedicated Hosts** - Your instances run on a dedicated physical server. More visibility how instances are placed on server. Dedicated Hosts can help reduce costs by letting you use existing server-bound software licenses and address corporate compliance and regulatory requirements.
  - Can be purchased On-Demand (hourly)
  - Can be purchased as a Reservation for up to 70% off the On-Demand price.
  - Uses Cases
    - Useful for regulatory requirements that may not support multi-tenant virtualisation.
    - Great for licensing which doesn't support multi-tenancy or cloud.

#### Security Groups

- A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic.
- If you don't specify a Security Group, the EC2 instance is linked to the default Security Group.
- Changes to a security groups rules take effect immediately and are automatically applied to all instances associated with that group.
- When you create a New Security Group
  - All inbound traffic is blocked by default - so we enable some IP and ports using Security Groups.
    - To let All IPs in `0.0.0.0/0`. To let a single IP address in `X.X.X.X/32` (32 means this ip address)
    - Common Ports: Linux (port 22) and Microsoft - RDP (port 3389)
  - All outbound traffic is allowed.
    - Common Ports: HTTP (80) and HTTPS (443)
- Cardinality: N Security Group <-> N EC2 Instance
  - You can have any number of EC2 instances within a security group.
  - You can have multiple Security Groups attached/assigned to EC2 instances.
- Security Groups vs ACL
  - Security Groups are STATEFUL, when you create an inbound rule and an outbound rule is automatically created. However, NACL's are STATELESS, when you create an inbound rule and an outbound rule is not automatically created.
  - Security Groups are only permisse, you can specify allows rule, but not deny rules. You CANNOT block specific IP's/Port's using Security Groups instead use Network Access Control Lists.

#### EC2 Enhanced Networking

- Elastic Network Interface (ENI) is a virtual network card, which you attach to EC2 instance in same AZ which is used to ensure a good network performance.
  - Enhanced Networking provides higher bandwidth, higher packets per second performance consistently lower it into instance latencies, and there's no additional charge for using
- Types:
  - Elastic Network Adapter (ENA) for C4, D2, and M4 EC2 instances, Upto 100 Gbps network speed.
  - Intel 82599 Virtual Function (VF) Interface for C3, C4, D2, I2, M4, and R3 EC2 instances, Upto 10 Gbps network speed. Old instance types.
  - Elastic Fabric Adapter (EFA) is ENA with additional OS-bypass functionality, which enables HPC and Machine Learning applications to bypass the operating system kernel and communicate directly with EFA device resulting in very high performance and low latency. for M5, C5, R5, I3, G4, metal EC2 instances.

#### EC2 Placement Groups Strategy

- A way of placing EC2 Instances so that instances are spread across the underlying hardware to minimise failures.
- AWS recommend homogeneous instances within clustered placement groups.
- Placement group names need to be unique within your account
- Only certain types of instances can be launched in a placement group (Compute Optimised, GPU, Memory Optimised, Storage Optimised)
- You can’t merge placement groups, but you can move an existing instance into a placement group (the instance must be in the stopped state befpre moving it)
  - Move or remove can only be done via AWS Console (an instance using the AWS CLI or AWS SDK).
- There is no charge associated with creating placement groups
- Types (A Clustered placement group can't span multiple AZ's, others can)
  - **Cluster** - Grouping instances close together within a **single Availability Zone**, Same Rack. It is used to achieve low Network latency & high throughput, High Performance Computing (HPC). Recommended you have the same type on instances in the cluster.
  - **Spread** - Opposite to clustered placement group. Instance are placed o Different AZ, Distinct Rack. It used for Critical Applications that requires to be seperated on each other to ensure High Availability in case of failure. Spread placement groups can span multiple Availability Zones.
  - **Partition** - EC2 creates partitions by dividing each group into logical segments. Each partition has its own set of racks, network and power source to help isolate the impact of a hardware failure. Same or Different AZ, Different Rack (or Partition), Distributed Applications like Hadoop, Cassandra, Kafka etc

#### AMI (Amazon Machine Image)

- Customized image of an EC2 instance, having built-in OS, softwares, configurations, etc.
- AMI's can be created from both Volumes and Snapshots.
  - You can create an AMI from EC2 instance and launch a new EC2 instance from AMI.
- AMI are built for a specific region and can be copied across regions

### Elastic Load Balancing (ELB)

- Designed to help balance the load of incoming traffic by distributing it across multiple targets/destinations.
  - Target group (ALB o CLB) can have one or more EC2 instances, IP Addresses, lambda functions.
- It makes the traffic Scale and Fault Tolerant (It can balance load across one or more Availability Zones)
- Internal Load Balancers are load balancers that are inside private subnets
- Load Balancers have their own static DNS name (e.g. <http://myalb-123456789.us-east-1.elb.amazonaws.com>) — you will NEVER be given an IP address
- If you need the IPv4 address of your end user, look for the `X-Forwarded-For` header.
- `Health Checks`
  - Instances monitored by ELB are reported as; InService, or OutofService
  - Health Checks check the instance health by talking to it.
  - `504 Error` means that the gateway has timed out. This means that the application not responding within the idle timeout period.
- Advanced Load Balancers Theory
  - `Stickiness` (a.k.a. Session Affinity):
    - Allows you to bind a users session to a specific instance, ensuring all requests in that specific session are sent to the same instance.
    - Use Cases:
      - A user trying to visit a website behind a classic load balancer and essentially what's happening is it's just sending all the traffic to one EC2 instance. Answer: Disable Sticky session.
      - If you have got an EC2 instance or an application, where you're writing to an EC2 instance like local disk, then of course you would want to enable Sticky.
    - It works in CLB and ALB. (It doesn’t work with NLB)
  - `Cross Zone load Balancing`
    - It enables EC2 instances to get equal share of traffic/load across multiple AZs
    - Use Cases:
      - With No Cross Zone Load Balancing, we got a user and we are using Route 53 for our DNS, which is splitting of our traffic 50/50 and sending the requests to EC2's in two diff AZ's.
        Each AC has a Load Balancer, The first AZ has 4 EC2 instances and the second has only one EC2 instance. 
        - Because we don't have Cross Zone Load Balancing enabled - First AZ will split 50% to 4 instances and the second AZ receives 50% on 1 instance.
        - When we enable Cross Zone Load Balancing: The Load balancer will distribute the load evenly among instances on both AZ's.
      - We got a user and we are using Route 53 for our DNS, which is sending all the requests (100%) to a Load Balancer in AZ1, The first AZ1 has 4 EC2 instances and the second has only one EC2 instance.
        - Route 53's 100% traffic is sent to the only load balancer in US-EAST-1A and no traffic is being sent to US-EAST-1B.
        - In this scenario, we enable Cross Zone Load Balancing to distribute the traffic evenly between US-EAST-1A and US-EAST-1B
  - `Path Patterns` (path-based routing) → can direct traffic to different EC2 instances based on request URL (path). For Example: you can route general requests to one target group and requests to render images to another target group
    - Use Case: We got a user and we are using Route 53 for our DNS, which is sending all the requests (100%) to a Load Balancer in AZ1, The first AZ1 has 4 EC2 instances and the second has only one EC2 instance.
      - www.myurl.com should go to AZ1 and www.myurl.com/images should go to the media instances in AZ2. In this instance, we enable Path Patterns.

- Types of ELB

| Type                        | Protocol                                                   | OSI Layer                   |
| --------------------------- | ---------------------------------------------------------- | --------------------------- |
| Application Load Balancer   | HTTP, HTTPS, WebSocket                                     | Layer 7 (Application layer) |
| Network Load Balancer       | TCP, UDP, TLS                                              | Layer 4 (Transport layer)   |
| Gateway Load Balancer       | Thirdparty appliances, virtual applications e.g. firewalls | Layer 3                     |
| Classic Load Balancer (old) | HTTP, HTTPS, TCP                                           | Both Layer 7 and Layer 4    |

#### Application Load Balancer (ALB)

![Application Load Balancer](https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/elb/Product-Page-Diagram_Elastic-Load-Balancing_ALB_HIW%402x.cb3ce6cfd5dd549c99645ed51eef9e8be8a27aa3.png)

- Best suitable for protocol HTTP, HTTPS, WebSocket | Layer 7 (Application layer)
- Routes traffic based on request content (hostname, request path, params, headers, source IP etc.).
- Use Case: It is **Intelligent** and can send specific requests to specific servers.

#### Network Load Balancer (NLB)

![Network Load Balancer](https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/elb/Product-Page-Diagram_Elastic-Load-Balancing_NLB_HIW%402x.2f8ded8b565042980c4ad5f8ec57d6b2fafe54ba.png)

- Best suitable for protocol TCP, UDP, TLS | Layer 4 (Transport layer)
- Use case: when extreme performance is required: Handle **volatile workloads** and **extreme low-latency**. Static IP Address.

#### Gateway Load Balancer (GLB)

![Gateway](https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/elb/Product-Page-Diagram_Elastic-Load-Balancing_GWLB_HIW%402x.58547db68b537b4aa4b0cdf7e593a6415d588a09.png)

- Thirdparty appliances, virtual applications e.g. firewalls | Layer 3
- Automatically scales virtual appliances based on demand.

#### Classic Load Balancers (Previos Generation)

- It can operate at both Layer 7 (Application layer) and Layer 4 (Transport layer).
- Use Case: Test & Dev to keep costs low.
- It is not very intelligent — it can’t route traffic based on content like Application Load Balancers.

## ASG (Auto Scaling Group)

![ASG](https://d1.awsstatic.com/product-marketing/AutoScaling/aws-auto-scaling-how-it-works-diagram.d42779c774d634883bdcd0463de7bd86f6e2231d.png)

- Monitors and scales applications to optimise performance and costs.
- It Can be used across a number of different services including EC2 instances and Spot Fleets, ECS tasks, Aurora replicas and DynamoDB tables.
- Instances are created in ASG using Launch Configuration (Legacy) or Launch Template (Recommended option)
  - You can create ASG that launches both Spot and On-Demand Instances or multiple instance types using launch template, not possible with launch configuration.
  - You cannot change the launch configuration for an ASG, you must create a new launch configuration and update your ASG with it.
- You can add Lifecycle Hooks to ASG to perform custom action during:
  1. scale-out to run script, install softwares and send complete-lifecycle-action command to continue
  2. scale-in e.g. download logs, take snapshot before termination

### Scaling options

Auto Scaling offers both dynamic scaling and predictive scaling options:

#### Dynamic Scaling

- Dynamic scaling scales the capacity of your Auto Scaling group as traffic changes occur.
- Types Dynamic Scaling Policies => Increase and decrease the current capacity of the group based on:
  - `Target tracking scaling`: A Amazon CloudWatch metric and a target value (it can combine more than one target). Health checks are performed to ensure resource level is maintained.
    - Use Case: Keep the average aggregate CPU utilization of your Auto Scaling group at 40% (and request count per target of your ALB target group at 1000)
  - `Step scaling`: A set of scaling adjustments, known as _step adjustments_, that vary based on the size of the alarm breach.
    - CloudWatch alarm CPUUtilization (60%-80%)- add 1, (>80%) - add 3 more, (30%-40%) - remove 1, (<30%) - remove 2 more
  - `Simple scaling`: A `single scaling adjustment`, with a `cooldown period` between each scaling activity.
    - CloudWatch alarm CPUUtilization (>80%) - add 2 instances

#### Predictive scaling

Predictive is only available for EC2 auto scaling groups and the scaling can work in a number of ways:

- Set `Maximum Capacity`: You specify minimum and maximum instances or desired capacity required and EC2 autoscaling manages the progress of creating/terminating based on what you have specified. min <= desired <= max

- Scale Based on a `Schedule`: Scaling performed as a function of time to reflect forecasted load. For example, if you know there will be increased load on the application at 9am every morning you can choose to scale at this time.

- Scale based on `Load forecasting`: Auto Scaling analyses the history of your applications load for up to 14 days and then uses this predict to the load for the next 2 days.

### Lambda

- FaaS (**Function as a Service**), Serverless. You don’t have to worry about OS or scaling (scale on demand)
- Lambda function supports many languages such as Node.js, Python, Java, C#, Golang, Ruby, etc.
- It is cheaper than EC2. There is no charge when your code is not running. What determines price for Lambda?
  - Request Pricing (Free Tier: 1 million requests per month)
  - Duration Pricing and resource (memory) usage
  - Additional Charges: if your lambda uses other AWS services or transfers data. For example, If your lambda function reads and writes data to or from Amazon S3, you will be billed for the read/write requests and the data stored in Amazon S3
- You are charged based on number of requests (first million free), execution time usage. Cheaper than EC2.
- AWS Lambda integrates with other AWS services to invoke functions or take other actions (Check examples [here](https://aws.amazon.com/lambda/))
- ==> Method of Invocation:

  - `Lambda polling`: For services that generate a queue or data stream, you set up an event source mapping in Lambda to have Lambda poll the queue or a data stream.
    - Services: Amazon Managed Streaming for Apache Kafka, Self-managed Apache Kafka, Amazon DynamoDB, Amazon Kinesis, Amazon MQ, Amazon Simple Queue Service
  - `Event-driven`: Some services generate events (JSON documents) that can invoke your Lambda function.
    - Synchronous
      - Services: Elastic Load Balancing (Application Load Balancer), Amazon Cognito, Amazon Lex, Amazon Alexa, Amazon API Gateway, Amazon CloudFront (Lambda@Edge), Amazon Kinesis Data Firehose, AWS Step Functions
      - Common Use Case: Respond to incoming HTTP requests using API Gateway.
    - Asynchronous
      - Common Use Case
        - Services: Amazon Simple Storage Service, Amazon Simple Notification Service, Amazon Simple Email Service, AWS CloudFormation, Amazon CloudWatch Logs, Amazon CloudWatch Events, AWS CodeCommit, AWS Config, AWS IoT Events
        - In response to resource **lifecycle events**, such as with Amazon Simple Storage Service (Amazon S3).
        - **On a Schedule** with Amazon EventBridge (CloudWatch Events).

- Lambda limitations:
  - Execution time can’t exceed 900 seconds or 15 min
  - Min required memory is 128MB and can go till 10GB with 1-MB increment
  - `/temp` directory size to download file can’t exceed 512 MB
  - Max environment variables size can be 4KB
  - Compressed `.zip` and uncompressed code can’t exceed 50MB and 250MB respectively

## Application_Integration

Go to [Index](#index)

### SQS (Amazon Simple Queue Service)

![SQS](https://d1.awsstatic.com/product-page-diagram_Amazon-SQS%402x.8639596f10bfa6d7cdb2e83df728e789963dcc39.png)

- Fully managed, distributed Message Queue service that can be used for micro-services, distributed applications and serverless applications. In other words, a temporary repository for messages that are awaiting processing.
- It **decouples infraestructure** (acts like a buffer between) the software component producing/saving data and the component receiving data for processing.
- Specification for Standard SQS:
  - SQS guarantees that your messages will be processed at least once.
  - Can have unlimited number of messages waiting in queue
  - Default retention period is 4 days (min 1 min. and max 14 days)
  - Can send message upto 256KB in size (To send messages larger than 256 KB -up tp 2GB- using library allows you to send an Amazon SQS message that contains a reference to a message payload in Amazon S3)
  - Unlimited throughput and low latency (<10ms on publish and receive)
  - Can have duplicate messages (At least once delivery)
  - Can have out of order messages (best effort ordering)
- Consumer (can be EC2 instance or lambda function) **poll** the messages in batches (upto 10 messages) and delete them from queue after processing. If don’t delete, they stay in Queue and may process multiple times.
  - Polling types:
    - Short Polling (`ReceiveMessageWaitTimeSeconds` = 0) - Keeps polling queue looking for work, even if it’s empty.
    - Long Polling (`ReceiveMessageWaitTimeSeconds` > 0) - Reduces the number of empty responses by allowing Amazon SQS to wait until a message is available before sending a response to a ReceiveMessage request, helps to reduce the cost.
  - Visibility Timeout — Immediately after a message is received, it remains in the queue. Amazon SQS doesn't automatically delete the message because it is a distributed system
    - To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds.
    - Use Case: If you are getting messages delivered twice, the cause could be your visibility timeout is too low.

There are two types of queues: Standard & FIFO

#### Standard Queues

- Default queue type.
- Nearly unlimited number of API calls per second.
- Guarantees message delivered at least once.
- Occasionally more than one copy of a message might be delivered out of order. However, standard queues provide **best-effort ordering** which ensures that messages are generally delivered in the same order as they are sent.

#### FIFO Queues

- First in First Out => The **order in which the messages are sent is preserved**.
- Has high throughput
- Limits: support up to 3,000 transactions per API batch call.
- Processed exactly once and duplicates are not introduced to the queue.

### SNS (Amazon Simple Notification Service)

- Managed Messaging Service that allows you **push** (Instantaneous) messages on SNS topic and all topic subscribers receive those messages.
- Can group multiple recipients through topics.
- Highly available as all messages stored across multiple regions.
- One topic can support deliveries to multiple endpoint types - for example, you can group together iOS, Android and SMS recipients. When you publish once to a topic, SNS delivers appropriately formatted copies of your message to each subscriber.
- Inexpensive, pay-as-you-go model with no up-front costs.

#### A2A (PubSub model)

![A2A](https://d1.awsstatic.com/Product-Page-Diagram_Amazon-SNS_Event-Driven-SNS-Compute%402x.03cb54865e1c586c26ee73f9dff0dc079125e9dc.png)

- Allows for many-to-many messaging between distributed systems, microservices and other AWS Services
- Event driven.
- Decouple messages publishers from subscriber with a topic
- You can setup a Subscription Filter Policy which is JSON policy to send the filtered messages to specific subscribers.
- Subscribers can be Kinesis Data Firehose, SQS, HTTP, HTTPS, Lambda, Email, Email-JSON, SMS Messages, Mobile Notifications.

#### A2P

![A2P_1](https://d1.awsstatic.com/Product-Page-Diagram_Amazon-SNS-SMS%402x.f499caaae8a9877fbefb4d9cf4768d030dc282da.png)

![A2P_2](https://d1.awsstatic.com/Product-Page-Diagram_Amazon-SNS-Mobile-Push%402x.08ac920f6c0bcf10c713be9e423b13e6fd9bd50c.png)

- Lets you send messages to your **customers** with SMS texts, push notifications, and email.

### Amazon MQ

- Amazon managed Apache ActiveMQ
- Migrate an existing message broker using MQTT protocol to AWS.

## Storage

Go to [Index](#index)

### S3 (Simple Storage Service)

![](https://d1.awsstatic.com/s3-pdp-redesign/product-page-diagram_Amazon-S3_HIW.cf4c2bd7aa02f1fe77be8aa120393993e08ac86d.png)

- Storage service that is highly scalable, secure and performant
- It is OBJECT BASED storage (suitable for files). It does not allow to install Operation System (different with EBS for example)
- S3 Object is made up of
  - Key → Name of the object, full path of the object in bucket e.g. /movies/comedy/abc.avi
    - S3 console show virtual folders based on key.
  - Value → data bytes of object (photos, videos, documents, etc.)
  - Version ID - version object (if versioning is enabled)
  - Metadata
  - Sub-resources (Access Control Lists & Torrent)
- There is unlimited storage, but individual files uploaded can be from **0 bytes to 5TB**.
  - Best practice:  use multi-part upload for Object size > 100MB
  - When you upload a file to S3, you receive a HTTP `200` code if the file upload is successful.
- S3 is a UNIVERSAL NAMESPACE, so bucket names need to be globally unique. The reason why is because it creates a web address (DNS name) with the buckets name in it.
  - When you view Buckets you view them globally but you have buckets in individual regions.

```sh
https://<bucket-name>.s3.<aws-region>.amazonaws.com
or
https://s3.<aws-region>.amazonaws.com/<bucket-name>
```

- **S3 Consistency**
  - Delivers **strong read-after-write consistency for PUTS and DELETES** of objects, for both new objects and for updates to existing objects. This means once there is a successful write, overwrite or delete — the next read request automatically receives the latest version of the object.
  - Updates to a single key are atomic. For example, if you PUT to an existing key from one thread and perform a GET on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data.
- In S3 you pay for the following things:
  - Storage
  - Requests and Data Retrievals
  - Storage Management Pricing
  - Data Transfer Pricing
  - Transfer Acceleration
  - Cross Region Replication Pricing
- We can change storage class and encryption on the fly

#### Optional features

- Enable `S3 Versioning` and `MFA` delete features to protect against accidental delete of S3 Object.
- Enable `S3 Object Lock` to store object using write-once-read-many (WORM) model to prevent objects from being deleted or overwritten for a fixed amount of time (`Retention period`) or indefinitely (`Legal hold`).
  - Amazon S3 currently does not support enabling object lock after a bucket has been created.
  - Retention:
    - Each version of object can have different retention-period.
    - S3 has two types of retention mode:
      - Governance Mode → Users can’t overwrite , delete or alter the object version locked unless they have special permissions (permissions requires to be granted).
      - Compliance Mode → A protected object version can’t be overwritten or deleted by ANY user including the root user during its retention period.
- `Glacier Vault Lock` → enforce compliance controls on individual S3 Glacier vaults using a Vault Lock policy.
- You can host **Static websites on S3 bucket** consists of HTML, CSS, client-side JavaScript, and images. Requirements:
  - Enable Static website hosting and Public access for S3 to avoid 403 forbidden error.
  - Add CORS Policy to allow cross origin request.

```sh
https://<bucket-name>.s3-website[.-]<aws-region>.amazonaws.com
```

**Note**: It is not used for dynamic websites or websites which require database, for ex: Wordpress...etc.

- `S3 Select` or `Glacier Select` can be used to retrieve subset of data from S3 Objects using SQL query. S3 Objects can be CSV, JSON, or Apache Parquet. GZIP & BZIP2 compression is supported with CSV or JSON format with server-side encryption.
  - Allows you to save money on data transfer and increase speed.
- Using `Range` HTTP Header in a GET Request to download the specific range of bytes of S3 object, known as Byte Range Fetch.
- You can create `S3 event notification` to push events e.g. s3:ObjectCreated:\* to SNS topic, SQS queue or execute a Lambda function. It is possible that you receive single notification for two writes to non-versioned object at the same time. Enable versioning to ensure you get all notifications.
- Enable `S3 Cross-Region Replication` for asynchronous replication of object across buckets in another region.
  - Cross Region Replication REQUIRES versioning to be ENABLED on both SOURCE & DESTINATION bucket.
  - If enabled, existing objects are not replicated automatically, only subsequent updated files (new objects).
  - You can have this enabled for the entire bucket or just for specific prefixes.
  - Delete markers ARE NOT replicated.
- Enable `Server access logging` for logging object-level fields object-size, total time, turn around time, and Http referrer. Not available with CloudTrail.
- Use `VPC S3 gateway endpoint` to access S3 bucket within AWS VPC to reduce the overall data transfer cost.
- Enable `S3 Transfer Acceleration` for faster transfer (high throughput) to S3 bucket (mainly uploads).
  - Create CloudFront distribution with Origin Access Identity (OAI) pointing to S3 for faster cached content delivery (mainly reads) over long distances between your client and S3.
  - Restrict the access of S3 bucket through CloudFront only using Origin Access Identity (OAI). Make sure user can’t use a direct URL to the S3 bucket to access the file.
- Use AWS Athena (Serverless Query Engine) to perform analytics directly against S3 objects using SQL query and save the analysis report in another S3 bucket.
  - Use Case: one time SQL query on S3 objects, S3 access log analysis, serverless queries on S3, IoT data analytics in S3, etc.

#### S3 Tiered Storage (Storage Classes)

- You can upload files in the same bucket with different Storage Classes like S3 standard, Standard-IA, One Zone-IA, Glacier etc.
- You can setup `S3 Lifecycle Rules` to transition current (or previous version) objects to cheaper storage classes or delete (expire if versioned) objects after certain period of time.
  - Use Case: Transition from S3 Standard to S3 Standard-IA or One Zone-IA can only be done after 30 days.
  - You can also setup lifecycle rule to abort multipart upload, if it doesn’t complete within certain days, which auto delete the parts from S3 buckets associated with multipart upload.
- Princing per Storage type:
  - `S3 Glacier Deep Archive` is the cheapest.
  - `S3 Standard` is the most expensive, if you are going to use it — why not use `S3 Intelligent tiering` (same price), unless you have thousands or millions of objects.
    - Benefit of using S3 Intelligent tiering: it does give you access to the infrequently access — so you could save money!
    - Warning: If you have a lot of objects you are going to incur monitoring and automation charges.

| S3 Storage Class                   | Durability | Availability | AZ  | Min. Storage | Retrieval Time                                           | Retrieval fee |
| ---------------------------------- | ---------- | ------------ | --- | ------------ | -------------------------------------------------------- | ------------- |
| S3 Standard (General Purpose)      | 11 9’s     | 99.99%       | ≥3  |  N/A         | milliseconds                                             | N/A           |
| S3 Intelligent Tiering             | 11 9’s     | 99.9%        | ≥3  | 30 days      | millisecond                                              | N/A           |
| S3 Standard-IA (Infrequent Access) | 11 9’s     | 99.9%        | ≥3  | 30 days      | milliseconds                                             | per GB        |
| S3 One Zone-IA (Infrequent Access) | 11 9’s     | 99.5%        | 1   | 30 days      | milliseconds                                             | per GB        |
| S3 Glacier                         | 11 9’s     | 99.99%       | ≥3  | 90 days      | Expedite (1-5 mins), Standard (3-5 hrs), Bulk (5-12 hrs) | per GB        |
| S3 Glacier Deep Archive            |  11 9’s    |  99.99%      |  ≥3 |  180 days    | Standard (12 hrs), Bulk (48 hrs) per GB                  |

- `Standard`: General purpose storage for any type of frequently used data very high availability, and fast retrieval.
  - HA: Stored redundantly across multiple devices in multiple facilities and is designed to sustain the loss of 2 facilities concurrently.
- `Intelligent Tiering`: Analyze your Object’s usage and move them to the appropriate cost-effective storage class automatically, without performance impact.
  - Use case: automatic cost savings for data with unknown/changing access patterns.
- `Standard-IA` (Infrequently Accessed): Cost effective for infrequent access files which cannot be recreated.
  - For data that is not accessed very frequently — but once it is accessed it needs to be retrieved rapidly.
  - It is cheaper than standard S3, but you do get charged a retrieval fee.
- `One-Zone IA`(also called S3 RRS): Cost effective for infrequent access files which can be recreated.
  - Low cost option for data that is not accessed frequently and does not require the redundancy, if the zone fails, we loose the data.
  - Use case: re-creatable infrequently accessed data that needs milliseconds access.
- `Glacier`: Cheaper choice to Archive Data. Retrival time configurable from minutes to hours
- `Glacier Deep Archive`: Cheapest choice for Long-term storage of large amount of data for compliance. Retrival time configurable but slower than `Glacier`, strating from 12 hours.

#### Sharing S3 buckets Across Accounts

For multiple accounts within the same organisation, to share S3 buckets among account:

- Bucket policy & IAM — applies to entire bucket, but programmatic access only.
- Using bucket ALCs & IAM — can apply to individual objects — programatic access only.
- Cross Account IAM roles — programatic and console access.

#### S3 Security

- By default newly created buckets are private, but you can make them public if needed, for example - you would need to make it public for static web hosting purposes.

#####  Access

###### Access Control lists (deprecated)

- Access Control Lists can be for **individual files**. Can grant basic read and write permissions at an object level (not just whole bucket)
- For example: use if there is a file in a bucket you don’t want everyone to have access to.

###### Bucket policy (recommended)

- S3 Bucket Policies are JSON based policy for complex access rules at user, account, folder, and object level
- Bucket policies are **bucket wide**. This works at budget levels not individual file level.

###### S3 Signed URLS

- Used to secure content so only authorised people are able to access (upload or download object data) temporarly it.
- It can be generated from CLI or SDK (can’t from web) and has an LIMITED LIFETIME (e.g. 5 min).

```sh
aws s3 presign s3://mybucket/myobject --expires-in 300
```

- Use Case: when not using CloudFront (Different from CloudFront signed urls) and user have direct access to S3.
- Issues a request as the IAM user who creates the pre-signed URL (Same permissions).

##### Encryption

- `Encryption at Rest (Client Side)` — client encrypt and decrypt the data before sending and after receiving data from S3.
- `Encryption in Transit` — encrypting network traffic (between client and S3) using SSL/TLS.
- `Encryption at Rest (Server Side)` — Encrypting the data which is stored. Can be achieved by:
  - `SSE-S3`: S3 Managed Keys (SSE-S3), AWS Managed Keys.
  - `SSE-KMS`: AWS Key Management Service(SSE-KMS) AWS & you manage keys together.
  - `SSE-C`: Customer provided keys — give AWS you own keys that you manage.
- To meet PCI-DSS or HIPAA compliance, encrypt S3 using SSE-C and Client Side Encryption.

#### S3 Versioning

- It acts **like a backup tool** that stores all versions of an object (even writes & deletes).
  - If you delete a file it will still show up in versioning with the delete marker on it.
- When enabled on your bucket it cannot be disabled — only suspended.
- It is possible to integrate it with lifecycle rules.
- If you mark a single file as public and then upload a new version of it — the new version is private.
- The size of your S3 bucket is the sum of all files and all versions of those files.
- Versioning's MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security.

#### S3 Performance

S3 Has extremely low latency

##### Performance limitations

- If you are using KMS (SSE-KMS) to encrypt your objects in S3, you must keep in mind the KMS limits.
  - When you upload a file, you will call `GenerateDataKey` in the KMS API.
  - When you download a file, you will call `Decrypt` in the KMS API.
- Uploading/Downloading will count towards the KMS per second quota, which could affect performance
  - Region-specific, however, it's either 5,500, 10,000 or 30,000 requests our second.
  - accessed through a Network File System (NFS) mount point

##### Improving Performance

- `S3 Prefix` is the part between the bucket name and the filename. You can get better performance by spreading your reads across different prefixes.

  - Use Case: By default, you can get the first byte out of S3 within 100-200 milliseconds. You can also achieve a high number of requests: 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix. Spreading your reads across different prefixes. For Example, If you are using two prefixes, you can achieve 11,000 requests per second.

```sh
mybucketname/folder1/subfolder1/myfile.jpg >  /folder1/subfolder1 is the prefix
```

- `Multipart Uploads` -> It splits your file into parts and uploads them in Parallel

  - Recommended for files over 100MB
  - Required for files over 5GB

- For download this is call `S3 Byte Range Fetches` — Parallelises download by specifying byte ranges, which speeds up downloads and can download partial amounts of info.

#### S3 Storage Gateway

- Is a hybrid cloud storage service for **connecting on-premises software applications with cloud based storage**.
- Can be downloaded as a Virtual Machine Image and installed in your datacenter.
- Has low latency as it caches data in the local VM or gateway hardware appliance.
- Storage Gateways Type
  - **1.** File Gateway (protocol NFS & SMB).
    - Stores objects directly in s3.
    - Utilises standard storage protocols with NFS & SMB.
    - Use case: on-premise backup to the cloud
  - **2.** Volume Gateway (ISCSI block protocol)
    - Presents your applications with disk volumes using ISCSI block protocol.
    - Stores/manages on-premise data in S3.
    - It allows you to take point-in-time snapshots using AWS Backup and stores them in EBS (Only captures changed blocks).
    - Types of Volume Gateways:
      - Stored Volumes — Entire Dataset is stored on site and is asynchronously backed up to S3. Store you primary data locally so there is low latency to the entire dataset and then asynchronously backs up that data to S3.
      - Cached Volumes — Entire Dataset is stored on S3 and the most frequently accessed data is cached on site. Uses s3 as your primary storage while retaining frequently accessed data locally. Minimise need to scale your on-premise infrastructure.
  - **3.** Tape Gateway (VTL)
    - Durable, cost effective archiving.
    - Is a way of replacing physical tapes with a virtual tape interface in AWS without changes existing backup workflows.

###  Instance Store

- Instance Store is an **Ephemeral/temporal** block-based storage physically attached to an EC2 instance
  - **Data persists on instance reboot, data doesn’t persist on stop or termination**
- It can be attached to an EC2 instance only when the instance is launched and cannot be dynamically resized
- Deliver very low-latency and high random I/O performance

### EBS (Elastic Block Store)

![EBS AWS diagram](https://d1.awsstatic.com/product-marketing/Storage/EBS/Product-Page-Diagram_Amazon-Elastic-Block-Store.5821c6ee4297f3c01cba37e304922451c828fb04.png)

- EBS is Persistent and High Available storage volumes for EC2
  - Data persists on EC2 stop or termination
    - When we terminate EC2 instance, it removes EBS volumes automatically.
  - Each Amazon EBS volume is automatically replicated within it's Availability Zone to protect you from component failure.
- It is Block-based storage: It needs to be **mounted to an EC2 instance within the same Availability Zone (Region)** (EBS Volume think like a "USB stick")
  - 1 EBS - 1 EC2. It can be attached to only one EC2 instance at a time in the same AZ (different from EFS ==> 1 EFS - 1..N EC2).
  - 1 EC2 - 1..N EBS. Can attach multiple EBS volumes to single EC2 instance. Data persist after detaching from EC2
- EBS Snapshots
  - It is an **incremental** backup of EBS Volume at a point in time saved into Amazon S3
    - Incremental => only blocks that have changed since your last snapshot (first snapshots takes longer)
  - Snapshots can be taken while the instance is running but ...
    - To create a snapshot for Amazon EBS volumes that serve as a root devices — best practice to terminate it first.
  - Snapshots can be shared with other AWS accounts or made public.
    - You can share snapshots, but only if they are unencrypted.
  - Mechanism to move Volumen data (EBS) to a different AZ location (EBS volume cannot be mount to an EC2 into a different AZ directly)
    - To move an EC2 volume from one region to another, take a snapshot of it, create an AMI from the snapshot and then **copy the AMI from one region to other**. Then use the copied AMI to launch the new EC2 instance in the new region.
    - An AMI's can be created from both Volumes and Snapshots.
- EBS volumen can be edited after instance is launched, it supports dynamic changes in live production volume e.g. volume type, volume size, and IOPS capacity without service interruption
- EBS Volume encryption:
  - All data at rest inside the volume is encrypted
  - All data in flight between the volume and EC2 instance is encrypted
  - All snapshots of encrypted volumes are automatically encrypted.
  - All volumes created from encrypted snapshots are automatically encrypted
  - Volumes created from unencrypted snapshots can be encrypted at the time of creation
- Types of EBS volumes:

**A/ SSD** for small/random IO operations, High IOPS means number of read and write operations per second, Only SSD EBS Volumes can be used as boot volumes for EC2

| SSD VolumeTypes                  |  Description                         |  Usage                                                                                                             |
| -------------------------------- | ------------------------------------ | ------------------------------------------------------------------------------------------------------------------ |
| General Purpose _SSD_ (gp2/gp3)  | Max 16000 IOPS                       | Balances price and performance and can be used for most workloads (boot volumes, dev environment, virtual desktop) |
| Provisioned IOPS _SSD_ (io1/io2) | 16000 - 64000 IOPS, EBS Multi-Attach | Mission critical business application, Databases (large SQL and NoSQL database workloads)                          |

**B/ HDD** (Hard Disk Drive) or Magnectic for large/sequential IO operations, High Throughput means number of bytes read and write per second

| HDD VolumeTypes                  |  Description                                                                 |  Usage                                                                                                                      |
| -------------------------------- | ---------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| Throughput Optimized _HDD_ (st1) | Low cost, frequently accessed, throughput intensive. Max 500 IOPS per volume | Big Data, Data warehouses, log processing                                                                                   |
| Cold _HDD_ (sc1)                 | Lowest cost, infrequently accessed. Max 250 IOPS per volume                  | Used for less frequently accessed workloads and when lowest storage cost is important. Common use could be for file servers |
| Magnetic (Standard)              | Max 40–200 IOPS per volume.                                                  | Previous generation hard disk drive typically used for infrequently accessed workloads.                                     |

- EBS Volumes with two types of RAID configuration:-

  - RAID 0 (increase performance) two 500GB EBS Volumes with 4000 IOPS - creates 1000GB RAID0 Array with 8000 IOPS and 1000Mbps throughput
  - RAID 1 (increase fault tolerance) two 500GB EBS Volumes with 4000 IOPS - creates 500GB RAID1 Array with 4000 IOPS and 500Mbps throughput

### EFS (Elastic File System)

![EFS AWS diagram](https://d1.awsstatic.com/legal/AmazonEFS/product-page-diagram_Amazon-EFS-Replication_HIW%402x.ccbabcc8777609fc0d23d7ff5ee1d52d5000dbf5.png)

- Fully managed, high scalable (elastic) and distributed (available) file storage that supports Network File Storage version 4 (NFSv4) and can be mounted to your EC2 instance.
  - Highly Scalable - can automatically scale from gigabytes to petabytes of data without needing to provision storage, growing and shrinking as you add/remove files. (you don't need to pre-provision storage like you do with EBS)
    - With burst mode, the throughput increase, as file system grows in size.
  - Highly Available - stores data redundantly across multiple Availability Zones.
  - Network File System (NFS) that can be mounted on and accessed concurrently in multiple AZs without sacrificing performance.
  - EFS file systems can be accessed by Amazon EC2 **Linux** instances, Amazon ECS, Amazon EKS, AWS Fargate, and AWS Lambda functions via a file system interface such as NFS protocol. (EBS only for EC2 instances)
- EFS is a POSIX-compliant file-based storage.
- EFS supports file systems semantics - strong read after write consistency and file locking.
- Native to Unix & Linux, but not supported on Windows instances.
- Only pay for what you use
- Performance Mode:
  - General Purpose for most file system for low-latency file operations, good for content-management, web-serving etc.
  - Max I/O is optimized to use with 10s, 100s, 1000s of EC2 instances with high aggregated throughput and IOPS, slightly higher latency for file operations, good for big data analytics, media processing workflow
- Use case: When you need scalable and resilient storage for linux instances
  - Share files, images, software updates, or computing across all EC2 instances in ECS, EKS cluster

### FSx for Windows

![FSx for Windows AWS diagram](https://d1.awsstatic.com/pdp-how-it-works-assets/Product-Page-Diagram_Managed-File-System-How-it-Works_Updated@2x.c0c4e846c0fca27e8f43bd1651883b21b4cc1eec.png)

- Fully managed, highly performant, native Microsoft Windows file system that supports SMB protocol & Windows NTFS. It also supports Microsoft Active Directory (AD) integration, ACLs, user quotas.
- Use case: When you need centralised storage for Windows-based applications such as Sharepoint, Microsoft SQL Server, Workspaces, IIS Web Server or any other native Microsoft Application.

### FSx for Lustre

![FSx for Lustre AWS diagram](https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-FSx-for-Lustre.097ed5e5175fa96e8ac77a2470151965774eec32.png)

- Fully managed and High performance file system for **fast processing of workload** with consistent **sub-millisecond latencies**, up to hundreds of gigabytes per second of throughput, and up to millions of IOPS.
- Lustre = Linux + Cluster is a **POSIX-compliant parallel linux file system**, which stores data across multiple network file servers.
- **Seamless integration with Amazon S3** (optional) - connect your S3 data sets to your FSx for Lustre file system, run your analyses, write results back to S3, and delete your file system.
- You can use **FSx for Lustre as hot storage** for your highly accessed files, and **Amazon S3 as cold storage** for rarely accessed files.
- FSx for Lustre provide two deployment options:
  - **Scratch file systems** - for temporary storage and short term processing
  - **Persistent file systems** - for high available & persist storage and long term processing
- Use case: When you need high speed or high capacity distributed storage for compute-intensive workloads, such as for Machine learning (ML), High performance computing (HPC), video processing, financial modeling, genome sequencing, and electronic design automation (EDA).

## Database

Go to [Index](#index)

### Database migration service (DMS)

- Transfer a database to another type (relational databases, data warehouses, NoSQL databases and other types of data stores.). It is valid for on-premise, in cloud (AWS o different vendro, Azure) or combination of both for Sources and Targets.
- It is a server in the AWS cloud that runs **Replication** software.
  - Create a source and a target endpoints
  - Schedule/Run a Replication Task (Replication Instance - VM) to move the data
  - No downtime (Source stays functioning the whole time during the migration)
- Types of migrations
  - Supports Homogenous Migrations — Identical e.g. oracle to oracle

![Homogeneos](https://d1.awsstatic.com/Product-Page-Diagram_AWS-Database-Migration-Service_Homogenous-Database-Migrations_Reduced%402x.053ebcf3f38feed093d6180bb7a351c5551a30a1.png)

  - Supports Hetrogenous Migrations — Different e.g. SQLServer to Aurora.
    - If you do this you will need to use a Schema Conversion Tool (SCT)

![Heterogeneos](https://d1.awsstatic.com/reInvent/reinvent-2022/data-migration-services/product-page-diagram_AWS-DMS_Heterogenous-Brief.e64d5fda98f36a79ab5ffcefa82b2735f94540ea.png)

### RDS (Relational Database Service)

![AWS RDS](https://d1.awsstatic.com/video-thumbs/RDS/product-page-diagram_Amazon-RDS-Regular-Deployment_HIW-V2.96bc5b3027474538840af756a5f2c636093f311f.png)

- AWS Managed Service to create High Available and Scalable **Relational** databases in the Cloud
  - It supports: PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, and Amazon Aurora
  - RDS runs on Virtual Machines (can’t log in to the OS or SSH in)
  - RDS is not serverless — (one exception Aurora Serverless)
- RDS Main Features
  - **High Available > Multi AZ Recovery** > User For Disaster Recovery Scenarios
    - Have a primary and secondary database, if you lose the primary database, AWS would detect and automatically update the DNS to point at the secondary database.
    - You can force a fail-over from AZ to another by rebooting the RDS instance.
  - **Scalable > Read Replicas** > Improves Performance
    - A Read Replica allows you to have read-only copies (Upto 5 Read replicas) of your production database. This is achieved by using Asynchronous replication so reads are eventually consistent. Every time you write to the main database, it is replicated in the secondary databases.
    - Read replicas can be distributed across multiple AZ, even in a different Region of your running RDS instance. You pay for replication cross Region, but not for cross AZ.
    - It must have automatic backups turned on in order to deploy a read replica.
    - You can have read replicas of read replicas (but watch out for latency)
    - Each read replica will have its own DNS end point.
    - Read replicas can be promoted to be their own databases. This breaks the replication.
- RDS Backups
  - Automated Backups
    - Enabled by default
    - Allows you to recover your database to any point in time within the specified retention period (Max 35 days)
    - Takes daily snapshots and stores transition logs
    - When recovering AWS will choose the most recent backup
    - Backup data is stored in S3
    - May experience latency when backup is being taken
    - Backups are deleted once you remove the original RDS instances
  - Database Snapshot
    - User-initiated, must be manually done by yourself
    - Stored until you explicitly delete them, even after you delete the original RDS instance they are still persisted (This is not the case with automated backups).
- Offers **encryption at rest** — done with KMS - Once your RDS instance is encrypted, as are its automated backups, read replicas, and snapshots.

### Amazon Aurora

![AWS Aurora](https://d1.awsstatic.com/Product-Page-Diagram_Amazon-Aurora_How-it-Works.b1c2b37e7548757780b195c6dcceb58511de5b1d.png)

- AWS fully managed relational database (Aurora Global Database) compatible with MySQL and PostgreSQL
  - Provides 5x better performance than MySQL
  - Provides 3x better performance than Postgres SQL
- Distributed: 2 copies of your data is contained in each Availability Zone (AZ) — minimum of 3 AZ’s and 6 copies.
  - Typically operates as a DB cluster consist of one or more DB instances and a cluster volume that manages cluster data with each AZ having a copy of volume.
    - Primary DB instance - Only one primary instance, supports both read and write operation
    - Replicas - Each Aurora DB cluster has built-in replication between multiple DB instances, you can choose between built-in features such as Aurora global databases or the traditional replication mechanisms for the MySQL or PostgreSQL DB engines
- Fault tolerant: It can handle the loss of up to 2 copies without affecting write ability and the lose of up to 3 copies of data without affecting read ability.
- Self-healing storage system (Data blocks and disks are continuously scanned for errors and repaired automatically.)
- Autoscaling for storage and computer capacity
  - Start with 10Gb, Scales in 10 GB increments to 64 TB (Storage Autoscaling)
  - Compute resources can scale up tp 32vCPUS and 244GB of Memory
- Backups with Aurora
  - Automated backups are always enabled on Amazon Aurora DB Instances. Backups do not impact performance.
  - You can also take snapshots with Aurora. This also does not impact on performance. Snapshots can be shared with other AWS accounts

#### Aurora Serverless

- On demand autoscaling configuration of Aurora
- Automatically starts up, shuts down, and scales based on app needs
- Used for simple, cost effective infrequently used, intermittent or unpredictable workloads
- Only pay for invocation.

### DynamoDB

![AWS Dynamo DB](https://d1.awsstatic.com/product-page-diagram_Amazon-DynamoDBa.1f8742c44147f1aed11719df4a14ccdb0b13d9a3.png)

- AWS proprietary, a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale.
- It is fully Managed and Serverless (no servers to provision, patch, or manage) database
- It supports both document (limit of 400KB item size. E.g. JSON documents, or session data.) and key-value data models. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, ad-tech, IoT, and many other applications.
- Stored on SSD Storage.
- Spread across 3 geographically distinct data centers.
- DynamoDB supports eventually consistent and strongly consistent reads. (eventual consistency is default)
  - Eventual Consistent Read (Default): Consistency across data within a second, meaning the response might not reflect the results of a just completed write operation, but if you repeat the read request again it should return the updated data. (Best Read Performance)
  - Strongly Consistent Reads: Returns the latest data. Results should reflect all writes that received a successful response prior to that read!

#### Streams

- Time ordered sequence of item level modifications in a table (stored up to 24 hours)
- Enable DynamoDB Streams to trigger events on database
- It can be integrated with lambda function for e.g. send welcome email to user added into the table.

#### Global Tables

- Fully managed, multi-active & multi-region database, that replicate your DynamoDB tables across selected regions
- Use DynamoDB Global Table to serve the data globally for distributed apps
- It is based on DynamoDB streams, thus DynamoDB Streams must enable first to create global table.
- Can be used for Disaster Recovery or high availability

#### Security in DynamoDB

- Encryption at rest using KMS
- Can use site to site VPN, direct connect (Dx) and IAM policies and roles
- Can implement fine grain access
- Can monitor on Cloud Watch and Cloud trail

####  DynamoDB Accelerator (DAX)

- Add DAX (DynamoDB Accelerator) cluster in front of DynamoDB to cache frequently read values and offload the heavy read on hot keys of DynamoDB, prevent `ProvisionedThroughputExceededException`
- Managed, highly available in memory cache for DynamoDB
- Has up to 10 times performance improvement
- Request time reduced to microseconds
- DAX manages all in-memory acceleration, so you don’t need to mange things like cache invalidations
- Compatible with Dynamo API calls

### ElastiCache

![AWS ElasticCache](https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_how-it-works.ec509f8b878f549b7fb8a49669bf2547878303f6.png)

- Amazon ElastiCache is a fully managed, in-memory caching service supporting flexible, real-time use cases.
- It allows you to deploy, operate & scale in-memory data stores(caching) in the cloud ==> Uses Cases:
  - Improves the performance of web applications, as it allows you to retrieve data fast from memory with high throughput and low latency. Use as distributed cache with sub millisecond performance
  - primary data store for use cases that don't require durability like session stores, gaming leaderboards, streaming, and analytics
- There are two types of in-memory caching engines:
  - Memcached — designed for simplicity, so used with you need the simplest model possible.
  - Redis — works for a wide range of use cases and have multi AZ. You can also complete backups/restores of redis.
    - Use password/token to access data using Redis Auth
    - HIPAA Compliant
- Services capable of caching
  - CloudFront
  - API Gateway
  - ElasticCache
  - Dynamo DB Accelerator
- Caching is a balancing act between up-to-date accurate information and latency.
- The further up you cache in your architecture the better e.g. at CloudFront level instead of waiting to DB level.

### Redshift

![AWS Redshift](https://d1.awsstatic.com/Product-Page-Diagram_Amazon-Redshift%402x.6c8ada98ebf822d3ddc113e6b802abe08fd4a4d2.png)

- Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver the best price performance at any scale.
- It can be used for **Business Intelligence (BI)**, allowing integrations with tools like AWS Quicksight or Tableau for analytics
- Redshift uses Advanced compression: Has column compression — compress columns instead of rows because of similar data because similar data is stored sequentially on disk.
- Backups:
  - Enabled by default with a 1 day retention period (Maximum is 35 days)
  - Redshift always attempts to maintain at least three copies of your data (the original and replica on the compute nodes and a backup in Amazon S3).
  - Redshift can also asynchronously replicate your snapshots to S3 in another region for DR.
  - Only Redshift can delete these automated snapshots, you can’t delete them manually.
- Security Considerations
  - Encrypted in transit using SSL
  - Encrypted at rest using AES-256 encryption
  - By default Redshift takes care of key management
- Pricing — compute node hours, backups and data transfer
- It can be configured as follows:
  - Single Node (160Gb)
  - Multi-Node
    1. Leader Node (manages client connections and receives queries)
    2. Compute Node (store data and perform queries and computations). Up to 128 Compute Nodes.
- Availability
  - Currently only in 1 AZ (check AWS to confirm for the latest)
  - Can restore snapshots to new AZs in the event of an outage
- It supports Massive Parallel Processing (MPP): Amazon Redshift automatically distributes data and query load across all nodes. Amazon Redshift makes it easy to add nodes to your data warehouse and enables you to maintain fast query performance as your data warehouse grows.

### Amazon Kinesis

![Amazon Kinesis](https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/kinesis/Product-Page-Diagram_Amazon-Kinesis-Data-Streams.e04132af59c6aa1e9372cabf44a17749f4a81b16.png)

- Kinesis is a fully managed platform for collecting, processing, and analyzing streaming real-time data in the cloud (video, audio, logs, analytics etc.) and process/analyse that data in real time. Real-time data generally comes from IoT devices, gaming applications, vehicle tracking, click stream, etc.

#### Kinesis Video Streams

- Fully managed service that makes it easy to securely (Automatically encrypts it at rest) stream live video from devices to the cloud.
- Enables playback, analytics and machine learning on video data that has been ingested.

#### Kinesis Data Streams

- Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale.
  Automatically stores data and encrypts it at rest
- Takes data from a producer and passes it through a SHARD to the consumer
  - Producer can be Amazon Kinesis Agent, SDK, or Kinesis Producer Library (KPL)
  - Consumer can be Kinesis Data Analytics, Kinesis Data Firehose, or Kinesis Consumer Library (KCL)
  - Data Retention period from 24 hours (default) to 365 days (max).
  - Order is maintained at Shard (partition) level.

##### Shards

- A shard is a sequence of data records in a stream with a fixed unit of capacity.
- A shard can support 5 transactions per second for reads with a max 2mb read rate per second.
- Only data streams have shards. The total capacity of a stream is the sum of the capacities of its shards.

#### Kinesis Firehose

- Kinesis Data Firehose load data streams into AWS data stores such as S3, Amazon Redshift and ElastiSearch. Transform data using lambda functions and store failed data to another S3 bucket.
- Can compress, transform and batch data to minimise the amount of storage.
- Encrypts your data streams before loading.
- Pay for the volume of data that transmits through the service.

#### Kinesis Analytics

- Kinesis Analytics allows you to analyse streaming data in real time to gain actionable insights.
- Allows you to process and analyse data using standard SQL.
- Can gain realtime dashboards and create real time metrics.
- Can use with Data Streams or Firehose as the streaming source.

### Amazon EMR

![Amazon EMR](https://d1.awsstatic.com/products/EMR/Product-Page-Diagram_Amazon-EMR.803d6adad956ba21ceb96311d15e5022c2b6722b.png)

- Amazon EMR (EMR = Elastic MapReduce) is the industry-leading cloud **Big Data** platform for processing vast amounts of data using open-source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi and Presto.
  - With EMR, you can run petabyte-scale analysis at less than half the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark.
- EMR can be used to perform data transformation workloads - Extract, transform, load (ETL)
- Use case: Analyze Clickstream data from S3 using Apache Spark and Hive to deliver more effective ads
- The central component of Amazon EMR is the cluster. A cluster is a collection of Amazon EC2 instances. Each instance in the cluster is called a Node, each node has a role within the cluster, referred to as the node type (different software components is installed on each node type)

  - Master node: A node that manages the cluster. The master node tracks the status of tasks and monitors the Health of the cluster. Every cluster has a master node.
    - By default - log data is stored on the master node. Alternatively, it can be configured (only at the moment of creation not after) a cluster to periodically archive the log files to Amazon S3 (at five-minute intervals). This ensures the log files are available after the cluster terminates, whether this is through normal shutdown or due to an error.
  - Core node: A node with software components that **Runs tasks and Stores data**. Multi-node clusters have at least one core node.
  - Task node: A node with software components that **only runs tasks** and does not store data. Task nodes are optional.

### Neptune

![AWS Neptune](https://d1.awsstatic.com/products/Neptune/product-page-diagram_Amazon-Neptune%402x.8af655592b659339933079725a914c14cbc0d831.png)

- Amazon Neptune is a fully managed Graph Database service built for the cloud that makes it easier to build and run graph applications. Neptune provides built-in security, continuous backups, serverless compute, and integrations with other AWS services.
- Use case: high relationship data, social networking data, knowledge graphs (Wikipedia)

### OpenSearch (old ElasticSearch)

![OpenSearch](https://d1.awsstatic.com/product-marketing/Elasticsearch/product-page-diagram_Amazon-OpenSearch-Service_HIW%402x.f20d73b8aa110b5fb6ca1d9ebb439066a5e31ef5.png)

- It is a managed Elastic Search service Amazon for performing interactive Log Analytics, Real-time application monitoring, Website search, and more.
- OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. Amazon OpenSearch Service offers the latest versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), as well as visualization capabilities powered by OpenSearch Dashboards and Kibana (1.5 to 7.10 versions).
- Integration with Kinesis Data Firehose, AWS IoT, and CloudWatch logs
- Use case: Search, indexing, partial or fuzzy search

## Migration

Go to [Index](#index)

### AWS Snow Family

- AWS snow family are **Physical device** used for on-premises large scale data migration to S3 buckets and processing data **at low network locations**.
- Use case: Addresses a lot of the common challenges that typically comes with with large-scale data transfers, including high network costs, long transfer times, and security concerns.

| Family Member | Storage | RAM | Migration Type   | DataSync | Migration Size |
| ------------- | ------- | --- | ---------------- | -------- | -------------- |
| Snowcone      | 8TB     | 4GB | online & offline | yes      | GBs and TBs    |

![snowcone](https://d1.awsstatic.com/SnowconHIW122722.406d05c2ce372214190996db3bd52e17e15e4007.png)

| Family Member                   | Storage | RAM   | Migration Type | DataSync | Migration Size |
| ------------------------------- | ------- | ----- | -------------- | -------- | -------------- |
| Snowball Edge Storage Optimized | 80TB    | 80GB  | offline        | no       | Petabyte-scale |
| Snowball Edge Compute Optimized | 42TB    | 208GB | offline        | no       | Petabyte-scale |

![snowball](<https://d1.awsstatic.com/hiw_snowball%402x%20(3).afde317ee4d3d8abe9a7ecc4fe52fefb9f454683.png>)

- AWS Snowball Edge comes with on-board storage and compute power for select AWS capabilities. Snowball Edge can do local processing and edge-computing workloads in addition to transferring data between your local environment and the AWS Cloud.

| Family Member | Storage | RAM | Migration Type | DataSync | Migration Size |
| ------------- | ------- | --- | -------------- | -------- | -------------- |
| Snowmobile    | 100PB   | N/A | offline        | no       | Exabyte scale  |

![snowmobile](https://d1.awsstatic.com/Product-Page-Diagram_AWS-Snowmobile%402x.4f7215d254697f7cb01d2e7189b81cb660165260.png)

### AWS Storage Gateway

![storagegateway](https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_AWS-Storage-Gateway_HIW@2x.6df96d96cdbaa61ed3ce935262431aabcfb9e52d.png)

- Store gateway is a **hybrid cloud service** to connect on-premises applications (data) with cloud storage.
- Types:

| Storage Gateway  | Protocol   | Backed by                               | Use Case                                                                                                                             |
| ---------------- | ---------- | --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| File Gateway     | NFS & SMB  | S3 -> S3-IA, S3 One Zone-IA             | Store files as object in S3, with a local cache for low-latency access, with user auth using Active Directory                        |
| FSx File Gateway | SMB & NTFS | FSx -> S3                               | Windows or Lustre File Server, integration with Microsoft AD                                                                         |
| Volume Gateway   | iSCSI      | S3 -> EBS                               | Block storage in S3 with backups as EBS snapshots. Use **Cached Volume** for low-latency and **Stored Volume** for scheduled backups |
| Tape Gateway     | iSCSI VTL  | S3 -> S3 Glacier & Glacier Deep Archive | Backup data in S3 and archive in Glacier using tape-based process                                                                    |

### AWS DataSync

- Data transfer service for moving large amounts of data into AWS. Has built in security capabilities (e.g. encryption in transit)
- Use cases

**A/** FROM an on-premise data center (using NFS and SMB storage protocol) TO AWS Storage: S3 (any storage type) , EFS, or FSx for Windows, AWS Snowcone. => Installing AWS Data Sync Agent on a VM, Amazon s3 Outspots or Snowcone

![on premise](https://d1.awsstatic.com/Digital%20Marketing/House/Editorial/products/DataSync/Product-Page-Diagram_AWS-DataSync_On-Premises-to-AWS%402x.8769b9dea1615c18ee0597b236946cbe0103b2da.png)

**B/** BETWEEN AWS storage services (e.g. to replicate EFS to EFS)

![between aws](https://d1.awsstatic.com/Digital%20Marketing/House/Editorial/products/DataSync/Product-Page-Diagram_AWS-DataSync-to-AWS-Storage-Services%402x.c9ae72a5d796feed1fd562b968fc133f9e66eec2.png)

**C/** FROM other Public Clouds to AWS Storage Services => Installing AWS Data Sync Agent on a VM

![different cloud](https://d1.awsstatic.com/Digital%20Marketing/House/Editorial/products/DataSync/AWS-DataSync-CrossCloud-to-AWS-Storage-Services_1%402x.bf8d56bb81dce99407eed06593b961bcb893dc0f.png)

### AWS Backup

![backup](https://d1.awsstatic.com/products/backup/Product-Page-Diagram_AWS-Backup%402x.9a3f6d1b456ddadac992018c5b308bb1d9e8c055.png)

- AWS Backup to centrally manage and automate backup process for EC2 instances, EBS Volumes, EFS, RDS databases, DynamoDB tables, FSx for Lustre, FSx for Window server, and Storage Gateway volumes
- Use case: Automate backup of RDS with 90 days retention policy. (Automate backup using RDS directly has max 35 days retention period)

### Database Migration Service (DMS)

- It helps to migrate database (data storage) to AWS with source remain fully operational during migration, minimize the downtime
- It supports multiple combinations: Out of AWS => AWS, AWS => AWS, AWS => Out of AWS
  - Out of AWS can be On premises or a Different Cloud Provider
- How it works? You need to select EC2 instance to run DMS in order to migrate (and replicate) database from source => target
- Types:

**A/** Homogenous migrations (origin and target same technology) e.g. On-premise PostgreSQL => AWS RDS PostgreSQL

![dms_homogeneous](https://d1.awsstatic.com/Product-Page-Diagram_AWS-Database-Migration-Service_Homogenous-Database-Migrations_Reduced%402x.053ebcf3f38feed093d6180bb7a351c5551a30a1.png)

**B/** Heterogenous migrations (origin and target different technology) such as MS SQL to Amazon Aurora. It requires to run AWS SCT (Schema Conversion Tool) at source

![dms_heterogeneous](https://d1.awsstatic.com/reInvent/reinvent-2022/data-migration-services/product-page-diagram_AWS-DMS_Heterogenous-Brief.e64d5fda98f36a79ab5ffcefa82b2735f94540ea.png)

### AWS Application Migration Service (MGN)

![mgn](https://d1.awsstatic.com/pdp-headers/2022/application-migration/MGN-How-It-Works-Diagram_biggerfonts1.1cb6cd71af1796ed95842d71c7b7a588a81c442d.jpg)

- It helps on automating the conversion of your **Source servers (VM)** (VMware vSphere, Microsoft Hyper-V or Microsoft Azure) **to run natively on AWS**. It also simplifies application modernization with built-in and custom optimization options.
- AWS Application Migration Service (new) utilizes continuous, block-level replication and enables cutover windows measured in minutes
- AWS Server Migration Service (legacy) utilizes incremental, snapshot-based replication and enables cutover windows measured in hours.

## Networking

Go to [Index](#index)

### Amazon VPC (Virtual Private Cloud)

![vpc](https://d1.awsstatic.com/Digital%20Marketing/House/Hero/products/ec2/VPC/Product-Page-Diagram_Amazon-VPC_HIW.9c472d7f2eb39ab8bdd22aa3ab80be00cdd00d8f.png)

- A VPC is a logical separated section of AWS Cloud (your own datacenter in AWS) for an account to enable:
  - Launch instances
  - Assign custom IP address ranges
  - Configure route tables between subnets
  - Create internet gateway and attach it to our VPC
  - Much better security control over your AWS resources
  - Instance security groups
  - Subnet network access control lists (ACL's)
- VPCs are region specific they do not span across regions
  - Every region comes with default VPC.
  - You can create upto 5 VPC per Region by default (soflimit, it can be extended)
- Default VPC vs Custom VPC
  - Default VPC is user friendly, allowing you to immediately deploy instances.
  - All subnets in a default VPC have a route out to the internet.
  - Each EC2 instance has both a public and private IP address.
  - If you delete a default VPC, you can recover it now. Try not to delete it.
- Auto assigning a public IP Address is turned off by default, this will need to be updated if you want a public subnet.
- You are not charged for using a VPC, however you are charged for the components used within it e.g. gateway, traffic monitoring etc.
- One way to save costs when it comes to networking is to use private IP addresses instead of public IP addresses as they utilise the AWS Backbone network.
- If you want to cut all network costs, group all EC2 instances in same AZ and use private IP addresses.
- Types of tenancy: On set up of your VPC you will be asked to choose either:
  - Dedicated → Everything on dedicated hardware (Very expensive)
  - Default → multi-tenant share underlying hardware with other AWS customers
- Cost nothing: VPCs, Route Tables, NACLs, Internet Gateway, Security Groups, Subnets, VPC Peering
- Cost money: NAT Gateway, VPC Endpoints, VPN Gateway, Customer Gateway

#### Required Components

- VPC’s consist of an Internet gateway, Subnets, Route tables, Network Access Control Lists and Security Groups.
- When we create a VPC
  - Created by default: a Route Table, Network Access Control List and Security Group.
  - No created by default: Subnets and Internet gateway

##### A/ Subnet (No created by default)

- A range of IP addresses within a VPC.
  - You assign one CIDR block per Subnet within CIDR range of your VPC. Should not overlap with other Subnet’s CIDR in your VPC.
  - Amazon don’t allow /8 prefix as it is too large — the largest they allow is /16
  - Amazon always reserve 5 IP addresses within your subnets (First 4 IPs and the last IP): Network Address, Router Address, DNS Server Address, Broadcast address and 1 more for future use. For e.g. If you need 29 IP addresses to use, your should choose CIDR /26 = 64 IP and not /27 = 32 IP, since 5 IPs are reserved and can not use.
  - Enable Auto assign public IPv4 address in public subnets, EC2 instances created in public subnets will be assigned a public IPv4 address
  - If you have 3 AZ in a region then you create total 6 subnets - 3 private subnets (1 in each AZ) and 3 public subnets (1 in each AZ) for multi-tier and highly-available architecture. API gateway and ALB reside in public subnet, EC2 instances, Lambda, Database reside in private subnet.
- Each subnet is tied to one Availability Zone, one Route Table, and one Network ACL
  - A subnet can not span multiple availability zones. However an AZ can have multiple subnets.

###### CIDR block (Classless Inter-Domain Routing)

- It is an internet protocol address allocation and route aggregation methodology. CIDR block has two components - Base IP (WW.XX.YY.ZZ) and Subnet Mask (From /0 to /32)
- Examples - Base IP 192.168.0.0
  - 192.168.0.0/32 means 2 raised to (32-**32**) = **1 single IP**
  - 192.168.0.0/24 means 2 raised to (32-**24**) = 256 IPs ranging from 192.168.0.0 to 192.168.0.255 (last number can change)
  - 192.168.0.0/16 means 2 raised to (32-**16**) = 65,536 IPs ranging from 192.168.0.0 to 192.168.255.255 (last 2 numbers can change)
  - 192.168.0.0/8 means 2 raised to (32-8)= 16,777,216 IPs ranging from 192.0.0.0 to 192.255.255.255 (last 3 numbers can change)
  - 0.0 0.0.0.0/0 means 232-0= All IPs ranging from 0.0.0.0 to 255.255.255.255 (all 4 numbers can change)

##### B/ Internet Gateway (No created by default)

![Internet Gateway](https://docs.aws.amazon.com/images/vpc/latest/userguide/images/internet-gateway-basics.png)

- It is known as Internet gateway or Virtual Private Gateway
- 1 VPC <-> 1 Internet Gateway. Each Internet Gateway is associated with one VPC only, and each VPC has one Internet Gateway only (one-to-one mapping)
- Allows your VPC to communicate with the Internet. Internet Gateway allows public subnet access to the internet and accessible from internet
- For internet communication, you must set up a route in your route table that directs traffic to the Internet Gateway
- Performs network address translation for instances

##### C/ Route Table (Created by default)

- A set of rules (called routes) that are used to determine where network traffic is directed.
  - Each Route table route has Destination like IPs and Target like local, IG, NAT, VPC endpoint etc.
  - Allows subnets to talk to each other
- Each subnet in your VPC must be associated with a route table.
- Cardinality
  - 1 Subnet -> 1 Route Table. A subnet can only be associated with one route table at a time
  - N Subnet -> Same Roue Table. Multiple subnets can be associated with the same route table For e.g. you create 4 subnets in your VPC where 2 subnets associated with one route table with no internet access rules know as private subnets and another 2 subnets are associated with another route table with internet access rules known as public subnets
- By default subnets are associated with the Main route table, but this can be a security risk e.g. if you were to put a route out to the public internet in the route table all subnets would automatically be made public.
  - To resolve this — keep main route table as private and then have separate route tables that use the main one, but have additional routes.
- Public vs Private Subnet
  - Public subnet ==> It is a subnet that’s associated with a route table having **rules to connect to internet using Internet Gateway**.
  - Private subnet ==> It is a subnet that’s associated with a route table having **no rules to connect to internet using Internet Gateway**. When our Subnets connected to the Private Route Table need access to the internet, we set up a NAT Gateway in the public Subnet. We then add a rule to our Private Route Table saying that all traffic looking to go to the internet should point to the NAT Gateway.

##### D/ Network Access Control List (Created by default)

- Extra layer of security **for your VPC** (acts as a Firewall) as it can be used to control the traffic in and out of subnets.
- Similar to security groups, as they contain rules, but you can you can **block IP addresses** with a NACL (unlike Security Groups).
- NACL are **stateless**, when you create an inbound rule and an outbound rule is not automatically created. It means they can have separate inbound and outbound rules (unlike Security Groups).
- A NACL can be associated with many Subnets, but a subnet can only have one NACL
- PCs comes with a modifiable default NACL. By default, it allows all inbound and outbound traffic.
- You can create custom NACL. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.
- Each subnet within a VPC must be associated with only 1 NACL
  - If you don’t specify, auto associate with default NACL.
  - If you associate with new NACL, auto remove previous association
- Apply to all instances in associated subnet
- Rules
  - Support both Allow and Deny rules
  - Evaluate rules in number order, starting with lowest numbered rule. NACL rules have number(1 to 32766) and higher precedence to lowest number for e.g. #100 ALLOW <IP> and #200 DENY <IP> means IP is allowed
  - Each network ACL also includes a rule with rule number as asterisk *. If any of the numbered rule doesn’t match, it’s denies the traffic. You can’t modify or remove this rule.
  - Recommended to create numbered rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need to later on.

##### E/ Security Groups (Created by default)

- Control inbound and outbound traffic **at EC2 instance level**
- You can specify allows rule, but not deny rules. You can specify a source in security group rule to be an IP range, A specific IP (/32), or another security group.
- When you first create a security group, bu default
  - All outbound traffic is allowed.
  - All inbound traffic is blocked.
- **Stateful** when you create an inbound rule and an outbound rule is automatically created.
- Cardinality: N Security Group <--> N EC2 instance.
  - You can have any number of EC2 instances within a security group.
  - You can have multiple Security Groups attached/assigned to EC2 instances. Evaluate all rules before deciding whether to allow traffic. Meaning if you have one security group which has no Allow and you add an allow in another than it will Allow

#### Optional Components

##### NAT Gateway/Instances

![NAT vs IG](https://miro.medium.com/max/1400/1*gftv4LSqU_12kRqNwYISJw.webp)

- NAT gateways/instances provides **private subnets access to internet traffic**, but ensures internet traffic does not initiate a connection with the instances.
- The NAT gateway/instance must live in a public subnet and then for a private subnet to connect to it, the private subnet must have a route in its route table that directs traffic to it.
- Use Case: For example this can enable our EC2 Instances in a private subnet to go out and download software by communicating with our Internet Gateway.
- NAT Gateway/Instances works with IPv4

###### NAT Instances (legacy)

- NAT Instances are individual EC2 instances. Community AMIs exist to launch NAT Instances. Works same as NAT Gateway.
- NAT instances are managed by you.
- It can be associated with security groups to control inbound and outbound traffic.
- Since NAT Instances send and receive traffic from different sources/destinations, it can cause some issues as EC2 does source/destination checks automatically — so when using a NAT Instance you need to **disable source/destination checks** on the EC2 instance when creating it.

###### NAT Gateway (latest, best practice)

- NAT Gateways are preferred by enterprise as they are **highly available**, can **scale** and are **managed by AWS**.  It is a managed service which launches redundant instances within the selected AZ (can survive failure of EC2 instance)
- You can create an AZ independent architecture with Network Gateways to reduce the risks of failures. This can be done by creating a NAT Gateway in each AZ and then configuring the routing to ensure resources in the same NAT Gateway are in the same AZ.
- Can not be associated with security groups, but you can associate the resources behind the NAT Gateway with security groups.
- Automatically assigned public IP Address
- You don’t need to worry about disabling source & destination checks on the instance.

###### Bastion Host

- A Bastion host is used to **securely administer EC2 instances** in private subnet (using SSH or RDP). Bastions are called Jump Boxes in Australia.
- A NAT Gateway or a NAT instance is used to provide **internet traffic** to EC2 instances in a private subnets. **They cannnot be used as Bastion Host**.

##### VPC Peering

- VPC peering connect two VPC over a direct network route using private IP addresses
- Instances on peered VPCs behave just like they are on the same network
- Must have no overlapping CIDR Blocks
- VPC peering connection are 1 to 1 (not transitive) i.e. VPC-A peering VPC-B and VPC-B peering to VPC-C doesn’t mean VPC-A peering VPC-C
- Route tables must be updated in both VPC that are peered so that instances can communicate
- Can connect one VPC to another in same or different region. VPC peering in different region called as VPC inter-region peering
- Can connect one VPC to another in same or different AWS account

##### VPC Flow Logs

- Capture information about **IP traffic information** (not hostnames) entering and leaving interfaces in your VPC.
  - They allow you to monitor the traffic reaching your instances and can help you see if your security groups are restrictive enough.
- They can be created at 3 levels: VPC, Subnet, Network Interface level.
- You can publish these flow logs with CloudWatch or S3. Query VPC flow logs using Athena on S3 or CloudWatch logs insight.
- Flow logs do not impact latency or network throughput as they are collected outside the path of your network traffic.
- You can have flow logs for peered VPCs, but only if they are in same account.

##### Transit Gateway

- Allows transitive peering between VPCs and on-premises data centres through a central hub.
- Works on regional bases but can span multiple regions.
- Supports IP Multicast, so can distribute the same content to multiple specific destinations (NOT supported by any other service).
- Overall used to simplify network typology.

##### VPC endpoints

- Allows you to **privately connect a VPC to other AWS resources** and it is powered by Private Link.
- Instances in your VPC do not require public IP addresses to communicate with resources in the service. So traffic between your VPC and other services does not leave the Amazon network.
- Eliminates the need of an Internet Gateway and NAT Gateway for instances in public and private subnets to access the other AWS services through public internet
- Eliminates the need of an Internet Gateway and NAT Gateway for instances in public and private subnets to access the other AWS services through public internet.
- 2 types:
  - Interface endpoint → Attach an Elastic Network Interface (ENI) with a private IP address onto your EC2 instance for it to communicate to services using AWS network. It serves as an entry point for traffic destined to a supported service.
  - Gateway endpoints → Create it as a route table target for traffic to services, like NAT gateways — its supported for only S3 & Dynamo.

###### VPC Private Link

- Provides private connections between VPC’s, AWS services and on-premise networks.
- Best way to expose your VPC to hundreds or thousands of other VPC’s.
- Can secure your traffic and simplify network management.
- Doesn’t require VPC peering, route tables or NAT gateways
- Requires Network Load Balancer on the service VPC and an elastic network interface on the customer VPC.

### VPN CloudHub

- If you have multiple sites, each with its own VPN connection, you can use AWS VPN CloudHub to connect those sites together.
- Low cost easy to manage.
- Operates over public internet, but all traffic is encrypted.
- Hub and Spoke model

### AWS Direct Connect

- **Directly connects** your on-premise datacenter to an AWS VPC using a dedicated network connection over a standard ethernet **fiber-optic cable**.
- Provide 1GB to 100GB/s network bandwidth for fast transfer of data from on-premises to Cloud
- Benefits of using Direct Connect includes: reduced network costs and increase in bandwidth throughput.

| AWS VPN | AWS Direct Connect |
| ------------- | ------- |
| Over the internet connection    | Over the dedicated private connection   |
| Configured in minutes    | Configured in days |
| low to modest bandwidth   | high bandwidth 1 to 100 GB/s |

- Exam question: A VPN connection keeps dropping out because the amount of throughput, and what kinds of things could you do to solve that? Answer: Direct Connect

### AWS VPN

- AWS Site-to-Site VPN connection is created to communicate between your remote network and Amazon VPC over the internet
- **VPN connection**: A secure connection between your on-premises equipment and your Amazon VPCs.
- **VPN tunnel**: An encrypted link where data can pass from the customer network to or from AWS. Each VPN connection includes two VPN tunnels which you can simultaneously use for high availability.
- Customer gateway: An AWS resource which provides information to AWS about your customer gateway device.
- Customer gateway device: A physical device or software application on customer side of the Site-to-Site VPN connection.
- Virtual private gateway: The VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You use a virtual private gateway or a transit gateway as the gateway for the Amazon side of the Site-to-Site VPN connection.
- Transit gateway: A transit hub that can be used to interconnect your VPCs and on-premises networks. You use a transit gateway or virtual private gateway as the gateway for the Amazon side of the Site-to-Site VPN connection.

### Amazon API Gateway

- It is at a fully managed service to Create and Manage APIs that acts as a front door for back-end systems running on EC2, AWS Lambda, etc. It makes easy for developers to publish, maintain, monitor and secure APIs at any scale.
- API Gateway Types - HTTP, WebSocket, and REST
- API Gateway has caching capabilities to increase performance
- API Gateway is low cost and scales automatically
- Allows you to track and control usage of API. Set throttle limit (default 10,000 req/s) to prevent from being overwhelmed by too many requests and returns `429 Too Many Request` error response.
- You can log results to CloudWatch
- If you are using Javascript/AJAX that uses multiple domains with API Gateway, ensure that you have enable CORS on API Gateway.
- CORS is enforced by the client (Browser)

### AWS Global Accelerator

![global_accelerator](https://d1.awsstatic.com/product-page-diagram_AWS-Global-Accelerator%402x.dd86ff5885ab5035037ad065d54120f8c44183fa.png)

- It is a service which you create accelerators to improve availability and performance of your applications for **global users**.
  - How? It directs traffic to optimal endpoints over the AWS Global network to avoid congestion.
  - First you create global accelerator, which provisions two anycast static IP addresses.
  - Then you register one or more endpoints with Global Accelerator. Each endpoint can have one or more AWS resources such as NLB, ALB, EC2, S3 Bucket or Elastic IP.
- You can control traffic using traffic dials. This is done within the endpoint group.
- You can control weighting to individual endpoints using weights (how much traffic is routed to each endpoint)
- Within endpoint, global accelerator monitor health checks of all AWS resources to send traffic to healthy resources only

### Amazon CloudFront

![cloudfront](https://docs.aws.amazon.com/images/AmazonCloudFront/latest/DeveloperGuide/images/how-you-configure-cf.png)

- It is a global service.
- It is a Content Delivery Network (CDN) uses AWS edge locations to cache and securely deliver cached content (such as images and videos) based on the geographic locations of the user, the origin of the webpage and a content delivery server (Requests to content are automatically routed to nearest geographical edge location). Advantages ==> Low latency and high transfer speeds.
  - Edge Location → Location where content will be cached (different to an AWS Region). They are not just read, you can also write to them.
  - Origin → Location where all the files THAT the CDN will distribute are stored — can be an S3 Bucket, EC2, ELB etc (Any type of AWS resource)
  - Distribution → Name of the CDN, which consists of a collection of edge locations. There are two types:
    - Web Distributions which are used for websites (Download Data)
    - RTMP Distributions which are used for streaming media (Stremming Access)
  - Invalidations → these can be files or subfolders that you can select to not be on the edge locations. Useful when you need to remove a file from an edge cache before it expires
  - Versioning → can be used to serve a different version of a file under a different name.
- Objects are cached for the Time To Live (TTL) - default 24 hours.
  - If requested resources does not exist on CloudFront — it will query the original server and then cache it on the edge location. Next requests get a cached copy from the Edge Location instead of downloading it again from the server until TTL expires.
  - It is possible to clear cached objects, however you will incur a charge.
- Can integrate with AWS Shied, Web Application Firewall and Route 53 to advance security (to protect from layer 7 attacks).
- It supports Geo restriction (Geo-Blocking) to whitelist or blacklist countries that can access the content.

#### Restricting Access to CloudFront: Signed URL ir Signed Cookies

- It used to restrict access to the resource to certain people so that it is only accessible through CloudFront and not directly through the AWS resource.
  - Example: Netflix - Option in AWS CloudFront is "Restrict Viewer access (Use Signed URL's or Signed cookies)")
  - If your origin is EC2, then use CloudFront Signed URL.
  - If your origin is S3, then use S3 signed URL instead of CloudFront Signed URL. (REVIEW THIS)
- You can restrict access using signed URLs or Signed Cookies.
  - A signed URL is for individual files, 1 files = 1 URL.
  - A signed cookie is for multiple file, 1 cookie = multiple files.
- When we create a signed URL or signed cookie, we attach a policy. The policy can include:
  a. URL expiration.
  b. IP ranges
  c. Trusted Signers (which AWS accounts can create signed URL's)

##### Features of a signed url

- The signed url (key pair) is account wide & managed by the root user.
- Has an associated policy statement (JSON) specifying restrictions on the URL.
- Contains additional information e.g. expiration date/time.
- Can have different origins and can utilise caching features.

### Amazon Route 53

![route53](https://d1.awsstatic.com/Route53/product-page-diagram_Amazon-Route-53_HIW%402x.4c2af00405a0825f83fca113352b480b19d9210e.png)

- Route 53 is AWS’s highly available, universal (not region specific) and scalable DNS service.
- Route 53 allows you to perform Domain Registration, DNS routing and also Health Checking.
  - If you want to use Route 53 for domain purchased from 3rd party websites (example GoDaddy).
    - AWS - You need to create a Hosted Zone in Route 53
    - 3 party DNS provider - update the 3rd party registrar NS (name server) records to use Route 53.
- It also works well with other AWS services — it allows you to connect requests to your infrastructure such as to EC2 instances, ELBs or S3 buckets.
- Private Hosted Zone is used to create an internal (intranet) domain name to be used within Amazon VPC. You can then add some DNS record and routing policy for that internal domain. That internal domain is accessible from EC2 instances or any other resource within VPC.
- There is a default limit of 50 domain names. However, this limit can be increased by contacting AWS support.

#### Terminology

- Internet Protocol (IP) → is a numerical label assigned to devices and used by computers to identify each other on a network.
- Domain Name System (DNS) → used to convert human friendly domain names into IP addresses.
- Domain Registrars → authority that can assign domain names
- Start of Authority Record (SOA) → type of resource record that every DNS must begin with, it contains the following information:
  - Stores the name of the server supplying the data
  - Stores the admin zone
  - Currently version of data file
  - Time to live
- Name Server (NS) records→ used by top level domain servers to direct traffic to the content DNS server. It specifies which DNS server is authoritative for a domain.
- A Records (Address Record) → type of DNS record, used by computer to translate a logical domain name to an IP address.
- Time To Live (TTL) → length of time the DNS record is cached on the server for in seconds. Default is 48 hours.
- Canonical Name (CName)→ It is used to resolve one domain name (hostname) to another (Map to a reference). Only works with subdomains e.g. something.mydomain.com
- Alias Record (A or AAAA) → Similar to CName but can be used:
  - At the top node of a DNS namespace, also known as the zone apex (a naked domain name) e.g. example.com
  - Points hostname to an AWS Resource like ALB, API Gateway, CloudFront, S3 Bucket, Global Accelerator, Elastic Beanstalk, VPC interface endpoint etc.
  - Works with both root-domain and subdomains

#### DNS Record: Routing Policy

In order for Route 53 to respond to queries, you need to define one of the following routing policies:

- **Simple** You can only have one record with multiple IP addresses. If you specify multiple values in a record, Route 53 returns all values to the user in a random order — so you never know which EC2 you are hitting and it can be shuffled on refreshed!
  - You can't have any health checks.
- **Weighted** Split traffic based on different custom proportions you assign.
  - You can set health checks on individual record sets. If a recordset fails a health check it will be removed from Route53 until it passes the health check.
  - Example: you can set 10% of your traffic to go to US-EAST-1 and 90% to EU-WEST-1.
- **Latency** Allows you to route your traffic based on the lowest network latency for your end user (ie which region will give them the fastest response time).
  - Example: create 3 DNS records with region us-east-1, eu-west-2, and ap-east-1.
- **Failover** to route traffic from Primary to Secondary (DR scenario) in case of failover (active/passive set-up)
  - It is mandatory to create health check for both IP and associate to record. The traffic goes to main site when its healthy and then can route traffic to the secondary site when the main one becomes unhealthy.
  - Example create 2 DNS records for primary site in EU-WEST-2 and secondary (DR) IP in AP-SOUTHEAST-2.
- **Geolocation** to route traffic to specific IP based on user geolocation (select Continent or Country).
  - For this you need to create separate record sets for each required location. It also requires a default (select Default location) policy in case there’s no match on location.
  - For Example: You might want all queries from Europe to be routed to a fleet of EC2 instances that are specifically configured for European customers. These servers may have the local language of European customers and all prices are displayed in Euros.
- **Geoproximity** to route traffic to specific IP based on user geolocation AND location of your resources.
  - You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.
  - To use Geoproximity Routing, you must use Route 53 traffic flow.
- **Multivalue Answer** configure Amazon Route 53 to return multiple values, such as IP addresses for your web servers, in response to DNS queries.
  - You can specify multiple value for almost any record, but multivalue answer routing also lets you check the health of each resource, so Route 53 returns only values from healthy resources.
  - Similar to Simple Routing only you can put health checks on each record set so that only healthy resources are returned.
  - Use case: Your company hosts 10 web servers all serving the same web content in AWS. They want Route 53 to serve traffic to random web servers.
  - Example: create 3 DNS records with associated health check. Acts as client side Load Balancer, expect a downtime of TTL, if an EC2 becomes unhealthy.

#### DNS Failover

- Active-Active failover when you want all resources to be available the majority of time. All records have same name, same type, and same routing policy such as weighted or latency
- Active-Passive failover when you have active primary resources and standby secondary resources. You create two records - primary & secondary with failover routing policy

## Management_and_Governance

Go to [Index](#index)

### Amazon CloudWatch

![cloudwatch](https://d1.awsstatic.com/reInvent/reinvent-2022/cloudwatch/Product-Page-Diagram_Amazon-CloudWatch.095eb618193be7422d2d34e3abd5fdf178b6c0e2.png)

- CloudWatch is a monitoring & observability service for AWS resources (EC2, ALB, S3, Lambda, DynamoDB, RDS etc.) and applications to watch **performance**.
  - It can collect these Inputs: Metrics and Log files
  - It allows you to create these Outputs:
    - Dashboards: Creates dashboards to see what is happening with your AWS environment
    - Alarms: Allows you to set Alarms that notify you when particular metric thresholds are hit
    - Events: CloudWatch Events helps you to respond to state changes in your AWS resources.
    - Logs: CloudWatch Logs helps you to aggregate, monitor and store logs.

#### CloudWatch with EC2

- It can monitor EC2 at host level: CPU, Network, Disk, Status Check
- It Monitors every 5 mins by default (Can switch to every 1min by enabling detailed logs)
- You can terminate or recover EC2 instance based on CloudWatch Alarm

### AWS CloudTrail

![CloudTrail](https://d1.awsstatic.com/product-marketing/CloudTrail/product-page-diagram_AWS-CloudTrail_HIW.feb63815c1869399371b4b9cc1ae00e78ed9e67f.png)

- CloudTrail is used for governance, compliance & operational **auditing**, security analysis
  - It collect as Inputs: of all the actions taken on any user on Management Console, AWS service, CLI, or SDK across AWS infrastructure.
  - Can detect user behaviour patterns and also unusual activity. Use case: check in the CloudTrail if any resource is deleted from AWS without anyone’s knowledge.
- CloudTrail works per AWS account and is enabled per region.
  - It is enabled by default for all regions
  - It can consolidate logs using S3 bucket:
    - Turn on CloudTrail in paying account.
    - Create a bucket policy that allows cross-account access.
    - Turn on CloudTrail in the other accounts and use the bucket in the paying account.

### AWS CloudFormation

![CloudFormation](https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png)

- Allows you to provision AWS resources as code. Infrastructure as Code (IaC).
  - Create, update, or delete your stack of resources using CloudFormation Template (blue prints) as a JSON or YAML file.
- It simplifies infrastructure management and also makes it very easy to replicate infrastructure across different regions.
  - Use case: Use to setup the same infrastructure in different environment for e.g. SIT, UAT and PROD. Use to create DEV resources everyday in working hours and delete later to lower the cost
- Using CloudFormation itself is free, underlying AWS resources are charged

#### Template Sections

- There are ten valid sections a CloudFormation template can contain, however they are not all required:
  - Resources — specify the actual resources you want to create (REQUIRED)
  - Parameters — Any values that you want to pass into your template at run time. (Optional)
  - Rules — used to validate parameters passed into the stack (Optional)
  - Mappings — mapping of key value pairs that can be used to specify conditions (Optional)
  - Outputs — values that are displayed when you check the stacks properties (Optional)
  - Conditions — can control whether a resource is created or whether certain properties are assigned depending on a particular criteria. (Optional)
  - Format Version — Version that the template conforms to (Optional)
  - Description — Describes what the template is used for. This is optional, but if you use it, it needs to follow the Format Version.
  - MetaData — any additional info about the template. (Optional)
  - Transform — used for serverless applications, allows you to specify the SAM version to use (Optional)
- Allows DependsOn attribute to specify that the creation of a specific resource follows another
- Allows DeletionPolicy attribute to be defined for resources in the template
  - retain to preserve resources like S3 even after stack deletion
  - snapshot to backup resources like RDS after stack deletion
- Supports Bootstrap scripts to install packages, files and services on the EC2 instances by simple describing them in the template
- automatic rollback on error feature is enabled, by default, which will cause all the AWS resources that CF created successfully for a stack up to the point where an error occurred to be deleted.

#### Serverless Application Model (SAM)

- Is an open-source framework that extends CloudFormation so that it is optimised for serverless applications (e.g. Lambdas, API’s, databases etc.)
- It supports anything CloudFormation supports.
- Also uses templates to define resources and these templates are in a YAML format.
- Can run serverless applications locally using docker.

### AWS Elastic Beanstalk (PaaS)

![ElasticBeanstalk](https://d1.awsstatic.com/Product-Page-Diagram_AWS-Elastic-Beanstalk%402x.6027573605a77c0e53606d5264ec7d3053bf26af.png)

- Platform as a Service (PaaS)
  - It Makes it easier for developers to quickly deploy and manage applications without thinking about underlying resources
  - It Automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling and application health monitoring
- You can launch an application with following pre-configured platforms:
  - Apache Tomcat for Java applications,
  - Apache HTTP Server for PHP and Python applications
  - Nginx or Apache HTTP Server for Node.js applications
  - Passenger or Puma for Ruby applications
  - Microsoft IIS 7.5 for .NET applications
  - Single and Multi Container Docker
- You can also launch an environment with following environment tier:-
  - An application that serves HTTP requests runs in a web server environment tier.
  - A backend environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.
- It costs nothing to use Elastic Beanstalk, only the resources it provisions e.g. EC2, ASG, ELB, and RDS etc.

### AWS ParallelCluster

![ParallelCluster](https://d1.awsstatic.com/HPC2019/Product-Page-Diagram_Paralell-Cluster_How-It-Works.50eb43991f5baa11d4d7687ac8155ed7943341ef.png)

- Open-source cluster management tools **deploy and manage High Performance Computing (HPC) clusters** resources on AWS (VPC, subnet, cluster type and instance types.) using a simple text file for modeling.
- You have full control on the underlying resources.
- AWS ParallelCluster is free, and you pay only for the AWS resources needed to run your applications.

### AWS Step Functions (SF)

![AWSStepFunctions](https://d1.awsstatic.com/video-thumbs/Step-Functions/AWS_Step_Functions_HIW.bc3d2930f00dd0401269367b8e8617a7dba5915c.png)

- Build serverless visual workflow to orchestrate your Lambda functions
- You write state machine in declarative JSON, you write a decider program to separate activity steps from decision steps.

### AWS Simple Workflow Service (SWF)

- Amazon Workflow Service (Amazon SWF) is a web service that makes it easy to coordinate work across distributed application components. SWF enables applications for a range of use cases, including media processing, web application back-ends, business process workflows and analytics pipelines, to be designed as a coordination of tasks.
- Tasks represent invocations of various processing steps in an application which can be performed by executable code, web service calls, human actions and scripts.
- Exam Scenario: Any human interaction required in the service, think of SWF

### AWS Organization

![Organization](https://d1.awsstatic.com/diagrams/organizations-HIW.1870c83be9fdfc55680172a1861080a91b700fff.png)

- Global Services that lets you create new AWS accounts at no additional charge.
  - With accounts in an organization, you can easily allocate resources, group accounts, and apply governance policies to accounts or groups, consolidate billing across all accounts (single payment method)
  - It has a master account and multiple member accounts
  - Member Acccounts or Organization Units (OUs) are based on department, cost center or environment, OU can have other OUs (hierarchy)
  - You can apply Service Control Policies (SCPs) at OU or account level, SCP is applied to all user and roles in that account. SPC Deny take precedence over Allow in the full OU tree of an account for e.g. allowed at account level but deny at OU level is = deny
- Best practices with AWS Organizations.
  - Always enable multi-factor authentication on root or master account.
  - Always use strong and complex passwords on root account.
  - Paying account should be used for billing purposes only. Do not deploy resources into the paying account, into the root account or the master account,
  - Enable and disable AWS services using service control policies (SCPs) either on organisational units or on individual accounts.

### AWS OpsWorks

- It is a Managed **Configuration as Code** Service that lets you use Chef and Puppet to automate how server are configured, deployed, managed across EC2 instances using Code.
- OpsWork Stack let you model you application as a stack containing different layers, such as load balancing, database, and application server.

### AWS Glue

- Serverless, fully managed **ETL (extract, transform, and load)** service.
- AWS Glue Crawler scan data from data source such as S3 or DynamoDB table, determine the schema for data, and then creates metadata tables in the AWS Glue Data Catalog.
- AWS Glue provide classifiers for CSV, JSON, AVRO, XML or database to determine the schema for data

## Containers

Go to [Index](#index)

- ECR (Elastic Container Registry) is Docker Registry to pull and push Docker images, managed by Amazon.
- ECS (Elastic Container Service) is container management service to run, stop, and manage Docker containers on a cluster
- ECS Task Definition where you configure task and container definition
  - Specify ECS Task IAM Role for ECS task (Docker container instance) to access AWS services like S3 bucket or DynamoDB
  - Specify Task Execution IAM Role i.e. ecsTaskExecutionRole for EC2 (ECS Agent) to pull docker images from ECR, make API calls to ECS service and publish container logs to Amazon CloudWatch on your behalf
  - Add container by specifying docker image, memory, port mappings, healthcheck, etc.
- You can create multiple ECS Task Definitions - e.g. one task definition to run web application on Nginx server and another task definition to run microservice on Tomcat.
- ECS Service Definition where you configure cluster, ELB, ASG, task definition, number of tasks to run multiple similar ECS Task, which deploy a docker container on EC2 instance. One EC2 instance can run multiple ECS tasks.
- Amazon EC2 Launch Type: You manage EC2 instances of ECS Cluster. You must install ECS Agent on each EC2 instances. Cheaper. Good for predictable, long running tasks.
- ECS Agent The agent sends information about the EC2 instance’s current running tasks and resource utilization to Amazon ECS. It starts and stops tasks whenever it receives a request from Amazon ECS
- Fargate Launch Type: Serverless, EC2 instances are managed by Fargate. You only manage and pay for container resources. Costlier. Good for variable, short running tasks
- EKS (Elastic Kubernetes Service) is managed Kubernetes clusters on AWS

## References

After doing the course from [Digital Training](https://digitalcloud.training/aws-cheat-sheets/), I reviwed the following resources to make this summary:

- [Amazon Docs](https://aws.amazon.com/)
- [AWS Certified Solutions Architect Associate (SAA-C02) Exam Notes - Coding N Concepts](https://codingnconcepts.com/aws/aws-certified-solutions-architect-associate)
- [AWS_CCP_Notes/AWS_Solution_Architecture_Associate.txt at main · kasukur/AWS_CCP_Notes](https://github.com/kasukur/AWS_CCP_Notes/blob/main/AWS_Solution_Architecture_Associate.txt)
- [AWS Solution Architect Associate Exam Study Notes | by Chloe McAteer | Medium](https://chloemcateer.medium.com/aws-solution-architect-associate-exam-study-notes-b6c5884ee500)

Finally, I practiced the following Exam Tests `AWS Certified Solutions Architect Associate Practice` from Udemy.
