# AWS Solution Architect

## Index

1. [Global](#global)
2. [Security](#security)
3. [Compute](#compute)
4. [Application Integration](#application_integration)
5. [Storage](#storage)
6. [References](#references)

## Global

Go to [Index](#index)

### Infrastructure

* [30 Regions with 96 Availability Zones](http://clusterfrak.com/notes/certs/aws_saa_notes/#security-and-identity)
* Edge locations are CDN endpoints for CloudFront. Currently there are over 50 edge locations. Not the same as an AZ
* Maximum Response time for Business is 1 hour
* Services such as CloudFormation, Elastic Beanstalk, Autoscaling, and OpsWorks are free however resources generated by these services are not free

#### AWS Region

* AWS regions are physical locations around the world having cluster of data centers
* You need to select the region first for most of the AWS services such as EC2, ELB, S3, Lambda, etc.
* You can not select region for Global AWS services such as IAM, AWS Organizations, Route 53, CloudFront, WAF, etc.
* Each AWS Region consists of multiple, isolated, and physically separate AZs (Availability Zones) within a geographic area.

#### AWS AZ (Availability zones)

* An AZ is one or more discrete data centers with redundant power, networking, and connectivity
* All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking.
* Customer deploy applications across multiple AZs in same region for high-availability, scalability, fault-tolerant and low-latency.
* AZs in a region are usually 3, min is 2 and max is 6 for e.g. 3 AZs in Ohio are us-east-2a, us-east-2b, and us-east-2c.
* For high availability in us-east-2 region with min 6 instances required either place 3 instances in each 3 AZs or place 6 instances in each 2 AZs (choose any 2 AZs out of 3) so that it works normal when 1 AZ goes down.

### Tags

* Key/Value pairs attached to AWS resources
* Metadata (data about data).
* Sometimes can be inherited (Auto-scaling, CloudFormation, Elastic Beanstalk can create other resources)
* Resource Groups make it easy to group your resources using the tags that are assigned to them
  * You can group resources that share one or more tags
  * Resource groups contain info such as region, name, health checks

### Amazon Resource Name (ARN)

* Identifier (ID) for any AWS resource. They are globally unique
* begin with `arn:<partition>:<service>:<region>:<account_id>` and end with: resource
resource_type/resource, resource_type/resource/qualifier, resource_type/resource:qualifier, resource_type:resource and resource_type:resource:qualifier
* Examples:
  * `arn:aws:iam::123456789012:user/sri` <-- :: region is omitted because IAM is global
  * `arn:aws:s3:::my_awesome_bucket/image.png` <-- ::: - no region, no account id needed to identify an object in S3. all objects in S3 are globally unique
  * `arn:aws:dynamodb:us-east-1:123456789012:table/orders`
  * `arn:aws:ec2:us-east-1:123456789012:instance/*` - wildcard. EC2 is a regional service

### Consolidated Billing

* Accounts roll for customers:
  * Paying account is independent, can not access resources of the other accounts
  * Linked accounts are independent from one another
  * Currently there is a limit of 20 linked accounts for consolidated billing (soft limit)
  * One bill per AWS account
  * Easy to track charges and allocate costs between linked accounts
  * Volume pricing discount
  * Resources across all linked accounts are tallied, and billing is applied collectively to allow bigger discounts

### Best Practices

* Business Benefits of Cloud:
  * Almost 0 upfront infrastructure investment
  * Just in time Infrastructure
  * More efficient resource utilization
  * Usage based costing
  * Reduced time to market

* Technical Benefits of Cloud:
  * Automation - Scriptable infrastructure
  * Auto-Scaling
  * Proactive Scaling
  * More efficient development life cycle
  * Improved testability
  * DR and Business Continuity
  * Overflow the traffic to the cloud

* Design for Failure
  * Rule of thumb: Be a pessimist when designing architectures in the cloud
  * Assume things will fail, always design implement and deploy for automated recovery from failure
  * Assume your hardware will fail
  * Assume outages will occur
  * Assume that some disaster will strike your application
  * Assume that you will be slammed with more than the expected number of requests per second
  * Assume that with time your application software will fail too

* Decouple your components:
  * Think SQS
  * Build components that do not have tight dependencies on each other so that if one component dies, fails, sleeps, or becomes busy, the other components are built so they can continue to work as if no failure is happening. Build each component as a black box

### [Service Quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) (known before as Service Limits)

* Your AWS account has default quotas, formerly referred to as limits, for each AWS service. Unless otherwise noted, each quota is Region-specific. You can request increases for some quotas, and other quotas cannot be increased.

## Security

Go to [Index](#index)

### IAM (Global Service)

* AWS Identity and Access Management Service (AWS IAM) is used to securely control **individual and group access to AWS resources**.

![IAM](https://d1.awsstatic.com/product-marketing/IAM/iam-how-it-works-diagram.04a2c4e4a1e8848155840676fa97ff2146d19012.png)

* IAM is global (so region isn’t a factor)
* It offers centralized control over your AWS account. It enables shared access to your AWS account.
* Granular Permissions (can set different permissions for different people/ different resources)
* Includes Federation Integration which taps into Active Directory, Facebook, Linkedin, etc. for authentication
* Multi-factor authentication support
* Allows configuration of temporary access for users, devices and services
* It supports PCI DSS compliance. PCI DSS compliance just is basically a compliant framework that if you're taking credit card details, you need to be compliant with the framework. So IAM supports PCI DSS.
* Access entities:
  * `Users` End users such as people, employees of an organisation ... etc
    * IAM users are individuals who have been granted access to an AWS account.
    * New Users
      * They have NO permissions when first created, NO have access to any AWS services (they can only login to the AWS console) ==> Permission must be explicitly granted to allow a user to access an AWS service.
      * They are assigned Access Key ID & Secret Access Keys when first created => You can get to view these once. If you lose them, you have to regenerate them.
      * In order for a new IAM user to be able to log into the console, the user must have a password set
  * `Groups` are a collection of users, and can not have other groups. Groups allow you to define permissions for all the users within it.
  * `Roles` Assigned to AWS resources, specifying what the resource (such as EC2) is allowed to access on another resource (S3)
    * Adventanges
      * Roles are more secure than storing your access key and secret access key on individual EC2 instances.
      * Roles are easier to manage.
      * Roles can be assigned to an EC2 instance after it is created using both the console & command line.
    * Types of Roles
      * Service Roles (To assign permission to AWS Resources)
      * Cross account access roles: Used when you have multiple AWS accounts and another AWS account must interact with the current AWS account
      * Identity provider access roles : Identity Federation (including AD, Facebook etc) can be configured to allow secure access to resources in an AWS account without creating an IAM user account.

* `Policies` JSON Document that defines permissions (`Allow` or `Deny` access to an action that can be performed on AWS resources) for Access Entities (user, group or role)
  * Each statement matches an AWS API request
  * Anything that is not explicitly allowed is implicitly denied
    * **IAM Policy Evaluation Logic** ➔ Explicit Deny ➯ Organization SCPs ➯ Resource-based Policies (optional) ➯ IAM Permission Boundaries ➯ Identity-based Policies
  * If a resource has multiple policies — AWS joins them
  * The **Least Privilege Principle** should be followed in AWS, don’t give more permission than a user needs.
  * **IAM Permission Boundaries** to set at individual user or role for maximum allowed permissions
  * **Resource Based Policies** are supported by S3, SNS, and SQS.
  * Sections:
    * `Version` policy language version. `2012-10-17` is the latest version.
    * `Statement` container for one or more policy statements
    * `Sid` (optional) a way of labeling your policy statement
    * `Effect` set whether the policy Allows or Deny
    * `Principal` user, group, role, or federated user to which you would like to allow or deny access
    * `Action` one or more actions that can be performed on AWS resources
    * `Resource` one or more AWS resources to which actions apply
    * `Condition` (optional) one or more conditions to satisfy for policy to be applicable, otherwise ignore the policy.

```json
{
    "Version":"2012-10-17",
    "Statement":[{
        "Sid": "Deny-Barclay-S3-Access",
        "Effect":"Deny",
        "Principal": { "AWS": ["arn:aws:iam:123456789012:barclay"] },
        "Action": [ "s3:GetObject", "s3:PutObject", "s3:List*" ],
        "Resource": ["arn:aws:s3:::mybucket/*"]
    },{
        "Effect": "Allow",
        "Action": "iam:CreateServiceLinkedRole",
        "Resource": "*",
        "Condition": {
            "StringLike": {
                "iam:AWSServiceName": [
                    "rds.amazonaws.com",
                    "rds.application-autoscaling.amazonaws.com"
                ]
            }
        }
    }]
}
```

* `Root account` is created by default with full administrator. Root account is email address that you used to register your account
  * Best practices
    * Not to use the root account for anything other than billing (not login)
    * Always setup Multifactor Authentication on your root account.
* `Power user access` → Access to all AWS services except the management of groups and users within IAM

### Access AWS

#### IAM Users

* Options:
  * AWS Management Console - Use password + MFA (multi factor authentication)
  * AWS CLI or SDK - Use Access Key ID (~username) and Secret Access Key (~password)
  * AWS CloudShell - CLI tool from AWS browser console - Require login to AWS

* Temporary security credentials consist of the AWS access key ID, secret access key, and security token.
  * IAM can assign temporary security credentials to provide users with temporary access to services/resources.

* When creating a user's credentials, you can only see/download the credentials at the time of creation not after.
* Access Keys can be retired, and new ones can be created in the event that secret access keys are lost
* To create a user password, once the users have been created, choose the user you want to set the password for and from the User Actions drop list, click manage password. Here you can opt to create a generated or custom password. If generated, there is an option to force the user to set a custom password on next login. Once a generated password has been issued, you can see the password which is the same as the access keys. Its shown once only

#### None IAM Users

* Non-IAM user first authenticates from Identity Federation ==> Then provide a temporary token (IAM Role attached) generated by calling a AssumeRole API of `STS (Security Token Service)` ==> Non-IAM user access the AWS resource by **assuming IAM Role attached with token**.

##### Identity Federation

* SAML 2.0 (old) to integrate Active Directory/ADFS, use AssumeRoleWithSAML STS API
* Custom Identity Broker used when identity provider is not compatible to SAML 2.0, use AssumeRole or GetFederationToken STS API
* Web Identity Federation is used to sign in using well-known external identity provider (IdP), such as login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible IdP. Get the ID token from IdP, use AWS Cognito api to exchange ID token with cognito token, use AssumeRoleWithWebIdentity STS API to get temp security credential to access AWS resources
* AWS Cognito (`Amazon Cognito Federated Identities`) is recommended identity provider by Amazon
* Amazon Single Sign On gives single sign on token to access AWS, no need to call STS API

##### AWS Directory Service

* Directories store information about users, groups, and devices, and administrators use them to manage access to information and resources. Hierarchical database of users, groups, computers - trees and forests.

###### Compatible with Microsft Active Directory

* `Managed Microsoft Active Directory`
  * It is managed Microsoft Windows Server AD with trust connection to on-premise Microsoft AD.
  * Best choice when you need all AD features to support AWS applications or Windows workloads.
  * Easily migrate on-premise workloads as it is built on actual Microsoft AD, so does not require any replication of existing directory to the cloud.
  * Highly available as directories are deployed across multiple Availability Zones and failovers are detected automatically.
  * A common use case would be to extend your on-premise using AD Trust with AWS Managed Microsoft AD so that both your on-premises and cloud directories remain separated, but it allows your users access AWS as needed.

* `Simple AD`
  * Use Simple AD  is standalone AWS managed compatible AD powered by Samba 4 with basic directory features (Enables a subset of the features Managed Mircosoft AD)
  * You cannot connect it to on-premise AD.
  * Best choice for basic directory features.
  * Can be used for Linux workloads that need LDAP

* `AD Connector`
  * It is proxy service to redirect requests to on-premise Microsoft AD, without caching information in the cloud.
  * Best choice to use existing on-premise AD with compatible AWS services.
  * Can use multiple AD Connectors to spread the load to match performance needs
  * Cannot be used across different AWS accounts

###### No Compatible with Microsft Active Directory

* `Cloud Directory`
  * Cloud-native directories for organizing Hierarchies of data along **multiple dimensions** fully managed by AWS
  * Can have multiple hierarchies with hundreds/millions of objects
  * Some common use cases include: directories for organisational charts, course catalogs, and device registries.

* `Amazon Cognito`
  * It control uses authentication (sign-up and sign-in) + access (premissions) for **mobile and web applications**. Supports guest users.
  * The two main components of Amazon Cognito are:

a. `User pools`: User directories in Amazon Cognito. Options for authetication
      *Directly through Amazon Cognito.
      * Through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers.

  In any case, menber of the user pool have a directory profile that you can access through a Software Development Kit (SDK)

![Amazon Cognito, User pools](https://docs.aws.amazon.com/images/cognito/latest/developerguide/images/scenario-authentication-cup.png)

b. `Identity pools`: Cognito elements grant users temporary credentials to other AWS services (e.g., Amazon S3 and DynamoDB).

Steps: You first authenticate user using `Cognito User Pools` and then exchange token with `Cognito Identity Pools` which further use `AWS STS` to generate temporary AWS credentials to access AWS Resources.

* AWS Security Token Service (AWS STS) as a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users)

### AWS Key Management Service (KMS)

* AWS managed centralized key management service to create, manage and rotate customer master keys (CMKs) for encryption at REST. Provides you with a central place to manage all keys.
* Can integrate with most other AWS services to increase security and make it easier to encrypt your data.

![AWS Key Management Service (KMS)](https://d1.awsstatic.com/Security/aws-kms/Group%2017aws-kms.6dc3dbbbe5b75b46c4f62218d0531e5bed7276ce.png)

* You can enable automatic master key rotation once per year. Service keeps the older version of master key to decrypt old encrypted data.
* Allows you to control access to the keys using things like IAM policies or key policies.
* Encrypt/decrypt up to 4KB.
* Pay per API call.
* Validated under FIPS 140–2 (**Level 2**) security standard.
* Types of Customer Master Keys (CMKs)
  * `Customer Managed CMKs` (Dedicated to my account) → Keys that you have created in AWS, that you own and manage. You are responsible for managing their key policies, rotating them and enabling/disabling them.
    * You can create customer-managed `Symmetric` (single key for both encrypt and decrypt operations) or `Asymmetric` (public/private key pair for encrypt/decrypt or sign/verify operations) master keys
    * Symmetric CMKs
      * With symmetric keys, the same key is used to encrypt and decrypt
      * The key never leaves AWS unencrypted
      * Must call the KMS API to use a symmetric key
      * The AWS services that integrate with KMS use symmetric CMKs
    * Asymmetric CMKs
      * Asymmetric keys are mathematically related public and private key pairs.
      * The private key never leaves AWS unencrypted.
      * You can call the KMS API with the public key, which can be downloaded and used outside of AWS.
      * AWS services that integrate with KMS DO NOT support asymmetric keys.
  * `AWS Managed CMKs` (Dedicated to my account) → These are free and are created by an AWS service on your behalf and are managed for you. However, only that service can use them. Used by default if you pick encryption in most AWS services
  * `AWS Owned CMKs` (No Dedicated to my account) → owned and managed by AWS and shared across many accounts.

### AWS CloudHSM

* Dedicated cloud-based Hardware Security Module (HSM) for creating, using and managing your own encryption keys (cryptographic keys) in AWS.
* Conforms to FIPS 140–2 (**Level 3**) security standard
* No access to the AWS managed component and AWS does not have visibility or access to your keys.
* Integrate with your application using industry-standard APIs (No AWS APIs), such as PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG) libraries (there are no AWS APIs for HSM)
* CloudHSM runs within a VPC in your account
  * CloudHSM will operate inside its own VPC dedicated to CloudHSM
  * CloudHSM will project to ENI of customer VPC
* Keys are irretrievable if lost and can not be recovered.

![AWS CloudHSM](https://d1.awsstatic.com/whiteboard-graphics/products/CloudHSM/product-page-diagram_AWS-CloudHSM_HIW.76ce14889e22d8861a6a9fff0b5664516ed1bddd.png)

* Use case: Use KMS to create a CMKs in a custom key store and store non-extractable key material in AWS CloudHSM to get a full control on encryption keys
* Difference between KMS and CloudHSM
  * FIPS 140–2 Level 2 vs Level 3
  * We manage our keys
    * KMS (Multi Tenant) and CloudHSM (Single Tenant, dedicate h/w, Mutli-AZ cluster)

### AWS Systems Manager

* `Parameter Store` is Secure and centralized serverless storage of configuration and secrets: passwords, database details, and license code, API Keys
  * `Parameter` value can be type String (plain text), StringList (comma separated) or SecureString (KMS encrypted data)
  * `Use case`: Centralized configuration for dev/uat/prod environment to be used by CLI, SDK, and Lambda function
* `Run Command` allows you to automate common administrative tasks and perform one-time configuration changes on EC2 instances at scale
* `Session Manager` replaces the need for Bastions to access instances in private subnet

### AWS Secrets Manager

* Secret Manager is mainly used to store, manage, and rotate secrets (passwords) such as database credentials, API keys, and OAuth tokens.
* Apply the new key/passwords in RDS for you. Generate random secrets.
* Secret Manager has **native support to rotate database credentials of RDS databases** - MySQL, PostgreSQL and Amazon Aurora. Automatically rotate secrets.
* For other secrets such as API keys or tokens, you need to use the **lambda for customized rotation function**.

### AWS Shield

* AWS managed **Distributed Denial of Service (DDoS) protection service**.
* Protect against **Layer 3 and 4** (Network and Transport) attacks.
* `AWS Shield Standard` is automatic and free DDoS protection service for all AWS customers for CloudFront and Route 53 resources.
* `AWS Shield Advanced` is paid service (Advanced costs $3K per month per org) for enhanced DDoS protection for EC2, ELB, CloudFront, and Route 53 resources.

### AWS WAF

* **Web Application Firewall** add an extra layer of protection to your web applications or APIs against web attacks from common exploits, such as SQL injection or Cross-site scripting (XSS).
* You can deploy WAF on CloudFront, Application Load Balancer, API Gateway and AWS AppSync
* Protect against **Layer 7** (HTTP & HTTPS) attacks and block common attack patterns by setting up rules to control the traffic. How?
  * Setting up your own rules to control the traffic by either only allowing what you specify or only blocking what you specify. Alternatively, you can count the requests that match a certain pattern.
  * AWS also provides managed rules that you can use to get started quickly, these are fully pre-configured and cover things like the OWASP Top 10 Security risks.
  * Conditions are used in WAFs to specify when you want to allow/block requests. Below are some examples of conditions that you might:
    * Values on the request header
    * The country a request comes from
    * Specific IP addresses
    * Strings that appear in requests
    * Length of the request
    * Presence of SQL code
    * Presence of a script
* Pay for what you use, based on the number of rules you have and requests your applications receive.

![AWS WAF](https://d1.awsstatic.com/Product-Page-Diagram_AWS-Web-Application-Firewall%402x.5f24d1b519ed1a88b7278c5d4cf7e4eeaf9b75cf.png)

### AWS Firewall Manager

* Use AWS Firewall Manager to centrally configure and manage Firewall Rules across an Organization: AWS WAF rules, AWS Shield Advanced, Network Firewall rules, and Route 53 DNS Firewall Rules
* Use case: Meet Gov regulations to deploy AWS WAF rule to block traffic from embargoed countries across accounts and resources

![](https://d1.awsstatic.com/products/firewall-manager/product-page-diagram_AWS-Firewall-Manager%402x%20(1)1.ad6bf5281dc2c33c0493e9988e3504dd1590eaa2.png)

### AWS GuardDuty

* Read VPC Flow Logs, DNS Logs, and CloudTrail events. Apply machine learning algorithms and anomaly detections to discover threats
* Can protect against CryptoCurrency attacks

![AWS GuardDuty](https://d1.awsstatic.com/Security/Amazon-GuardDuty/Amazon-GuardDuty_HIW.057a144483974cb73ab5f3f87a50c7c79f6521fb.png)

### Amazon Inspector

* Automated Security Assessment service for **EC2 instances** by installing an agent in the OS of EC2 instance.
* Inspector comes with pre-defined rules packages:
  * `Network Reachability` rules package checks for unintended network accessibility of EC2 instances
  * `Host Assessment rules` package checks for vulnerabilities and insecure configurations on EC2 instance. Includes Common Vulnerabilities and Exposures (CVE), Center for Internet Security (CIS) Operating System configuration benchmarks, and security best practices.

![](https://d1.awsstatic.com/reInvent/re21-pdp-tier1/amazon-inspector/Amazon-Inspector_HIW%402x.c26d455cb7e4e947c5cb2f9a5e0ab0238a445227.png)

### Amazon Macie

* Managed service to discover and protect your **sensitive data** in AWS
* Can automatically discover **Personally Identifiable Information (PII)**  in your data and can alert you once identified (e.g. selected S3 buckets)
* Can produce dashboards, reporting and alerts
* Benefits of Macie
  * Cost efficiently discovers sensitive data at scale
  * Constant visibility of data security/privacy through alerts and dashboards
  * Can use used across AWS accounts through AWS Organisations.
  * Can help you meet regulatory compliance
  * Great for PCI-DSS(credit card payments) and preventing theft

![Amazon Macie](https://d1.awsstatic.com/reInvent/reinvent-2022/macie/Product-Page-Diagram_Amazon-Macie.a51550cca0a731ba2e4a26e8463ed5f5a81202e3.png)

### AWS Config

* Managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. Assess, audit, and evaluate configurations of your AWS resources in multi-region, multi-account
* You are notified via SNS for any configuration change
* Integrated with CloudTrail, provide resource configuration history
* Use case: Customers need to comply with standards like PCI-DSS (Payment Card Industry Data Security Standard) or HIPAA (U.S. Health Insurance Portability and Accountability Act) can use this service to assess compliance of AWS infra configurations

![AWS Config](https://d1.awsstatic.com/config-diagram-092122.974fe2a4cb6aae1fe564fdbbe30ab55841a9858e.png)

## Compute

Go to [Index](#index)

### EC2

* Infrastructure as a Service (IaaS) - Re-sizable (elastic) and secure virtual machine on the cloud.
  * Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change.
  * Gives you complete control of your computing resources including choice of storage, processor, networking and operating system.
* When you restart an EC2 instance, its public IP can change. Use `Elastic IP` to assign a fixed public IPv4 to your EC2 instance. By default, all AWS accounts are limited to five (5) Elastic IP addresses per Region.
* You need to create a key pair — public & private for asymmetric encryption.
* The EC2 Root volume is a virtual disk where the OS is installed, it can only be launched on SSD or Magnetic.
* Bootstrap scripts are code that gets ran as soon as your EC2 instance first boots up.
* EC2 Information Endpoints (can be obteined via `curl`):
  * `http://169.254.169.254/latest/meta-data` ==> Metadata Private & public IP
  * `http://169.254.169.254/latest/user-data` ==> user-defined data
* Use VM Import/Export to import virtual machine image and convert to Amazon EC2 AMI to launch EC2 instances
* Exams tips:
  * Termination protection is turned off by default, you must turn it on.
  * On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated. Any additional EBS volumes by default won't be deleted.
  * Root device volumes can be encrypted now (a popular exam topic)
  * EBS Root volumes of your DEFAULT AMI's CAN be encrypted. You can also use a third party tool (such as bit locker etc) to encrypt the root volume, or this can be done when creating AMI's (lab to follow) in the AWS console or using the API.
  * Additional volumes can be encrypted as well.
  * You must provision nitro-based EC2 instance to achieve 64000 EBS IOPS. Max 32000 EBS IOPS with Non-Nitro EC2.
  * EC2 Design
    * Place all the EC2 instances in same AZ to reduce the data transfer cost.
    * Design for failure. Have one EC2 instance in each availability zone.

#### EC2 Hibernate

* Allows you to hibernate your EC2 instances, so that you can stop them and pick back up where you left off again.
* It does this by saving the content from the in-memory state of the instance (RAM) to your EBS root volume.
* You can hibernate an instance only if it’s enabled for hibernation and it meets the hibernation prerequisites
* Useful for long running services and services that take long to boot.
* Can’t hibernate for more than 60 days
* Once in hibernation mode there is no hourly charge — you only pay for the elastic IP Address & other attached volumes
* Boots up a lot faster after hibernation as it does not need to reload the operating system.

#### EC2 Instance Types

You can choose EC2 instance type based on requirement for e.g. m5.2xlarge has Linux OS, 8 vCPU, 32GB RAM, EBS-Only Storage, Up to 10 Gbps Network bandwidth, Up to 4,750 Mbps IO Operations.

| Instance Class  | Usage Type  |  Usage Example  |
|--- |--- |--- |
|  T, M | General Purpose |  Web Server, Code Repo, Microservice, Small Database, Virtual Desktop, Dev Environment  |
|  C  | Compute Optimized  |  High Performance Computing (HPC), Batch Processing, Gaming Server, Scientific Modelling, CPU-based machine learning  |
|  R, X, Z  | Memory Optimized  |  In-memory Cache, High Performance Database, Real-time big data analytics |
|  F, G, P  | Accelerated Computing  |  High GPU, Graphics Intensive Applications, Machine Learning, Speech Recognition |
|  D, H, I | Storage Optimized  |  EC2 Instance Storage, High I/O Performance, HDFS, MapReduce File Systems, Spark, Hadoop, Redshift, Kafka, Elastic Search |

#### EC2 Pricing Types

* **On-Demand** - Pay a fixed rate by the hour (or by the second) with no commitment. Pay as you use, costly
  * Use Cases:
    1. Users that want the low cost and flexibility of Amazon EC2 without any up-front payment for long-term commitment.
    2. Applications with short term, spiky or unpredictable workloads that cannot be interrupted
    3. Applications being developed or tested on Amazon EC2 for the first time.
* **Reserved** - Provides you with a capacity reservation, and offer significant discount on the hourly charge for an instance, but it requires to have a Contracts for 1 - 3 year terms. Higher discount with upfront payments and longer contracts. However, you cant move between regions.
  * Uses Cases:
    1. Applications with steady or predictable usage.
    2. Applications that require reserved capacity.
    3. Users able to make upfront payments to reduce their total computing costs even further.
  * Types:
    * **Standard Reserved Instances**  Provides the most discount (up to 75% off). Unused instanced can be sold in AWS reserved instance marketplace
    * **Convertible Reserved Instances** up to 54% off. It can be exchanged for another Convertible Reserved Instance with different instance attributes e.g. you to change between instance types e.g. t1-t4 as long as its of greater or equal value
    * **Scheduled Reserved Instances** - reserve capacity that is scheduled to recur daily, weekly, or monthly, with a specified start time and duration, for a one-year term.
* **Spot Instances** -  Enables you to bid whatever price you want for instance capacity. when AWS has excess capacity it drops the price so people can use that capacity —but they can take it back at any time. You can set the price you are willing to pay and it will run when its below or at that price — if it goes above that price you lose it.
  * It provides up to 90% discount and typically used for apps with flexible start/end times, But don’t use for anything critical that needs to be online all the time. It can handle interruptions and recover gracefully.
  * Imp Note: If the spot instance is terminated by Amazon EC2, you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged for any hour in which the instance ran.
  * Uses Cases
    1. Applications that have flexible start and end times.
    2. Applications that are only feasible at very low compute prices.
    3. Users with urgent computing needs for large amounts of additional capacity.
  * Types
    * **Spot Blocks** can also be launched with a required duration, which are not interrupted due to changes in the Spot price.
    * **Spot Fleet**
      * Collection of spot instances and optionally on-demand instances. Attempts to launch a number of them together to meet a certain capacity within your price budget.
      * The allocation of spot instances depends on how they fulfil your spot fleet request from the possible pool of instances.
      * Strategies:
        * Lowest Price → This is the default strategy. Chooses the fleet pool with the lowest price.
        * Diversified → Distributed across all pools.
        * Capacity Optimised → Pool for optimal capacity for the number of instances launching.
        * InstancePoolsToUseCount → Distributed across the number of pools you specify — this can only be used with the lowest price option.

* **Dedicated Instance** - Your instance runs on a dedicated hardware provide physical isolation, single-tenant
* **Dedicated Hosts** - Your instances run on a dedicated physical server. More visibility how instances are placed on server. Dedicated Hosts can help reduce costs by letting you use existing server-bound software licenses and address corporate compliance and regulatory requirements.
  * Uses Cases
    1. Useful for regulatory requirements that may not support multi-tenant virtualisation.
  1. Great for licensing which doesn't support multi-tenancy or cloud.
  2. Can be purchased On-Demand (hourly)
  3. Can be purchased as a Reservation for up to 70% off the On-Demand price.

#### Security Groups

* A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic.
* If you don't sepcify a Securtiy Group, the EC2 instance is linked to the default Security Group
* Changes to Security Groups take effect immediately.
* When you create a New Security Group
  * All inbound traffic is blocked by default - so we enable some IP and ports using Security Groups.
    * To let All IPs in `0.0.0.0/0`. To let a single IP address in `X.X.X.X/32` (32 means this ip address)
    * Common Ports: Linux (port 22)
  * All outbound traffic is allowed.
    * Common Ports: Microsoft - RDP (port 3389), HTTP (80) and HTTPS (443)
* Changes to a security groups rules take effect immediately and are automatically applied to all instances associated with that group.
* Cardinality Security Group *-* EC2 Instance
  * You can have any number of EC2 instances within a security group.
  * You can have multiple Security Groups attached/assigned to EC2 instances.
* Security Groups vs ACL
  * Security Groups are STATEFUL, when you create an inbound rule and an outbound rule is automatically created. However, NACL's are STATELESS, when you create an inbound rule and an outbound rule is not automatically created.
  * Security Groups are only permisse, you can specify allows rule, but not deny rules. You CANNOT block specific IP's/Port's using Security Groups instead use Network Access Control Lists.

#### EC2 Enhanced Networking

* Elastic Network Interface (ENI) is a virtual network card, which you attach to EC2 instance in same AZ. ENI has one primary private IPv4, one or more secondary private IPv4, one Elastic IP per private IPv4, one public IPv4, one or more IPv6, one or more security groups, a MAC address and a source/destination check flag
  * While primary ENI cannot be detached from an EC2 instance, A secondary ENI with private IPv4 can be detached and attached to standby EC2 instance if primary EC2 becomes unreachable (failover)
* Elastic Network Adapter (ENA) for C4, D2, and M4 EC2 instances, Upto 100 Gbps network speed.
* Elastic Fabric Adapter (EFA) is ENA with additional OS-bypass functionality, which enables HPC and Machine Learning applications to bypass the operating system kernel and communicate directly with EFA device resulting in very high performance and low latency. for M5, C5, R5, I3, G4, metal EC2 instances.
* Intel 82599 Virtual Function (VF) Interface for C3, C4, D2, I2, M4, and R3 EC2 instances, Upto 10 Gbps network speed.

#### EC2 Placement Groups Strategy

* A way of placing EC2 Instances so that instances are spread across the underlying hardware to minimise failures.
* Placement group names need to be unique within your account
* Only certain instances can be launched in placement groups e.g compute optimised, CPU, memory optimised & storage optimised.
* You can’t merge placement groups, but you can move an existing instance into a placement group.
* There is no charge associated with creating placement groups

* **Cluster** - Grouping instances close together within a single Availability Zone, Same Rack. It is used to achieve low Network latency & high throughput, High Performance Computing (HPC). Recommended you have the same type on instances in the cluster.
* **Spread** - Opposite to clustered placement group. Instance are placed o Different AZ, Distinct Rack. It used for Critical Applications that requires to be seperated on each other to ensure High Availability in case of failure. Spread placement groups can span multiple Availability Zones.
* **Partition** - EC2 creates partitions by dividing each group into logical segments. Each partition has its own set of racks, network and power source to help isolate the impact of a hardware failure. Same or Different AZ, Different Rack (or Partition), Distributed Applications like Hadoop, Cassandra, Kafka etc

#### AMI (Amazon Machine Image)

* Customized image of an EC2 instance, having built-in OS, softwares, configurations, etc.
* You can create an AMI from EC2 instance and launch a new EC2 instance from AMI.
* AMI are built for a specific region and can be copied across regions

## Elastic Load Balancing (ELB)

* Designed to help balance the load of incoming traffic by distributing it across multiple targets/destinations.
  * Target group (ALB o CLB) can have one or more EC2 instances, IP Addresses, lambda functions.
* It makes the traffic Scale and Fault Tolerant (It can balance load across one or more Availability Zones)
* Internal Load Balancers are load balancers that are inside private subnets
* Load Balancers have their own static DNS name (e.g. <http://myalb-123456789.us-east-1.elb.amazonaws.com>) — you will NEVER be given an IP address
* If you need the IPv4 address of your end user, look for the `X-Forwarded-For` header.
* `Health Checks`
  * Instances monitored by ELB are reported as; InService, or OutofService
  * Health Checks check the instance health by talking to it.
  * `504 Error` means that the gateway has timed out. This means that the application not responding within the idle timeout period.
* Advanced Load Balancers Theory
  * `Stickiness` (a.k.a. Session Affinity):
    * Allows you to bind a users session to a specific instance, ensuring all requests in that specific session are sent to the same instance.
    * Use Cases:
      * A user trying to visit a website behind a classic load balancer and essentially what's happening is it's just sending all the traffic to one EC2 instance. Answer: Disable Sticky session.
      * If you have got an EC2 instance or an application, where you're writing to an EC2 instance like local disk, then of course you would want to enable Sticky.
    * It works in CLB and ALB. (It doesn’t work with NLB)
  * `Cross Zone load Balancing`
    * It enables EC2 instances to get equal share of traffic/load across multiple AZs
    * Use Cases:
      * With No Cross Zone Load Balancing, we got a user and we are using Route 53 for our DNS,    which is splitting of our traffic 50/50 and sending the requests to EC2's in two diff AZ's.
   Each AC has a Load Balancer, The first AZ has 4 EC2 instances and the second has only one EC2 instance.
        * Because we don't have Cross Zone Load Balancing enabled - First AZ will split 50% to 4 instances and the second AZ receives 50% on 1 instance.
        * When we enable Cross Zone Load Balancing: The Load balancer will distribute the load evenly among instances on both AZ's.
      * We got a user and we are using Route 53 for our DNS, which is sending all the requests (100%) to a Load Balancer in AZ1, The first AZ1 has 4 EC2 instances and the second has only one EC2 instance.
        * Route 53's 100% traffic is sent to the only load balancer in US-EAST-1A and no traffic is being sent to US-EAST-1B.
        * In this scenario, we enable Cross Zone Load Balancing to distribute the traffic evenly between US-EAST-1A and US-EAST-1B
  * `Path Patterns` (path-based routing) → can direct traffic to different EC2 instances based on request URL (path). For Example: you can route general requests to one target group and requests to render images to another target group
    * Use Case: We got a user and we are using Route 53 for our DNS, which is sending all the requests (100%) to a Load Balancer in AZ1, The first AZ1 has 4 EC2 instances and the second has only one EC2 instance.
      * www.myurl.com should go to AZ1 and www.myurl.com/images should go to the media instances in AZ2. In this instance, we enable Path Patterns.

* Types of ELB

| Type  | Protocol | OSI Layer |
|--- |--- |--- |
Application Load Balancer | HTTP, HTTPS, WebSocket | Layer 7 (Application layer) |
Network Load Balancer | TCP, UDP, TLS | Layer 4 (Transport layer) |
Gateway Load Balancer | Thirdparty appliances, virtual applications e.g. firewalls| Layer 3 |
Classic Load Balancer (old) | HTTP, HTTPS, TCP | Both Layer 7 and Layer 4 |

#### Application Load Balancer (ALB)

* Best suitable for protocol HTTP, HTTPS, WebSocket | Layer 7 (Application layer)
* Routes traffic based on request content (hostname, request path, params, headers, source IP etc.).
* Use Case: It is **Intelligent** and can send specific requests to specific servers.

![Application Load Balancer](https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/elb/Product-Page-Diagram_Elastic-Load-Balancing_ALB_HIW%402x.cb3ce6cfd5dd549c99645ed51eef9e8be8a27aa3.png)

#### Network Load Balancer (NLB)

* Best suitable for protocol TCP, UDP, TLS | Layer 4 (Transport layer)
* Use case: when extreme performance is required: Handle **volatile workloads** and **extreme low-latency**. Static IP Address.

![Network Load Balancer](https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/elb/Product-Page-Diagram_Elastic-Load-Balancing_NLB_HIW%402x.2f8ded8b565042980c4ad5f8ec57d6b2fafe54ba.png)

#### Gateway Load Balancer (GLB)

* Thirdparty appliances, virtual applications e.g. firewalls | Layer 3
* Automatically scales virtual appliances based on demand.

![Gateway](https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/elb/Product-Page-Diagram_Elastic-Load-Balancing_GWLB_HIW%402x.58547db68b537b4aa4b0cdf7e593a6415d588a09.png)

#### Classic Load Balancers (Previos Generation)

* It can operate at both Layer 7 (Application layer) and Layer 4 (Transport layer).
* Use Case: Test & Dev to keep costs low.
* It is not very intelligent — it can’t route traffic based on content like Application Load Balancers.

## ASG (Auto Scaling Group)

![ASG](https://d1.awsstatic.com/product-marketing/AutoScaling/aws-auto-scaling-how-it-works-diagram.d42779c774d634883bdcd0463de7bd86f6e2231d.png)

* Monitors and scales applications to optimise performance and costs.
* It Can be used across a number of different services including EC2 instances and Spot Fleets, ECS tasks, Aurora replicas and DynamoDB tables.
* Instances are created in ASG using Launch Configuration (Legacy) or Launch Template (Recommended option)
  * You can create ASG that launches both Spot and On-Demand Instances or multiple instance types using launch template, not possible with launch configuration.
  * You cannot change the launch configuration for an ASG, you must create a new launch configuration and update your ASG with it.
* You can add Lifecycle Hooks to ASG to perform custom action during:-
  1. scale-out to run script, install softwares and send complete-lifecycle-action command to continue
  2. scale-in e.g. download logs, take snapshot before termination

### Scaling options

Auto Scaling offers both dynamic scaling and predictive scaling options:

#### Dynamic Scaling

* Dynamic scaling scales the capacity of your Auto Scaling group as traffic changes occur.
* Types Dynamic Scaling Policies => Increase and decrease the current capacity of the group based on ...
  * Target tracking scaling: A `Amazon CloudWatch metric` and a `target value` (it can combine more than one target). Health checks are performed to ensure resource level is maintained.
    * Use Case: Keep the average aggregate CPU utilization of your Auto Scaling group at 40% (and request count per target of your ALB target group at 1000)
  * Step scaling: A set of scaling adjustments, known as `step adjustments`, that vary based on the size of the alarm breach.
    * CloudWatch alarm CPUUtilization (60%-80%)- add 1, (>80%) - add 3 more, (30%-40%) - remove 1, (<30%) - remove 2 more
  * Simple scaling: A `single scaling adjustment`, with a `cooldown period` between each scaling activity.
    * CloudWatch alarm CPUUtilization (>80%) - add 2 instances

#### Predictive scaling

Predictive is only available for EC2 auto scaling groups and the scaling can work in a number of ways:

* Set **Maximum Capacity**: You specify minimum and maximum instances or desired capacity required and EC2 autoscaling manages the progress of creating/terminating based on what you have specified. min <= desired <= max

* Scale Based on a **Schedule**: Scaling performed as a function of time to reflect forecasted load.
For example, if you know there will be increased load on the application at 9am every morning you can choose to scale at this time

* Scale based on **Load forecasting**: Auto Scaling analyses the history of your applications load for up to 14 days and then uses this predict to the load for the next 2 days.

### Lambda

* FaaS (Function as a Service), Serverless. You don’t have to worry about OS or scaling (scale on demand)
* Lambda function supports many languages such as Node.js, Python, Java, C#, Golang, Ruby, etc.
* It is cheaper than EC2. There is no charge when your code is not running. What determines price for Lambda?
  * Request Pricing (Free Tier: 1 million requests per month)
  * Duration Pricing and resource (memory) usage
  * Additional Charges: if your lambda uses other AWS services or transfers data. For example, If your lambda function reads and writes data to or from Amazon S3, you will be billed for the read/write requests and the data stored in Amazon S3
* You are charged based on number of requests (first million free), execution time  usage. Cheaper than EC2.
* AWS Lambda integrates with other AWS services to invoke functions or take other actions ==> Method of Invocation:
  * Lambda polling: For services that generate a queue or data stream, you set up an event source mapping in Lambda to have Lambda poll the queue or a data stream.
    * Services: Amazon Managed Streaming for Apache Kafka, Self-managed Apache Kafka, Amazon DynamoDB, Amazon Kinesis, Amazon MQ, Amazon Simple Queue Service
  * Event-driven: Some services generate events (JSON documents) that can invoke your Lambda function.
    * Synchronous
      * Services: Elastic Load Balancing (Application Load Balancer), Amazon Cognito, Amazon Lex, Amazon Alexa, Amazon API Gateway, Amazon CloudFront (Lambda@Edge), Amazon Kinesis Data Firehose, AWS Step Functions
      * Common Use Case: Respond to incoming HTTP requests using API Gateway.
    * Asynchronous
      * Common Use Case
        * Services: Amazon Simple Storage Service, Amazon Simple Notification Service, Amazon Simple Email Service, AWS CloudFormation, Amazon CloudWatch Logs, Amazon CloudWatch Events, AWS CodeCommit, AWS Config, AWS IoT Events
        * In response to resource **lifecycle events**, such as with Amazon Simple Storage Service (Amazon S3).
        * **On a Schedule** with Amazon EventBridge (CloudWatch Events).

* Lambda limitations:
  * Execution time can’t exceed 900 seconds or 15 min
  * Min required memory is 128MB and can go till 10GB with 1-MB increment
  * `/temp` directory size to download file can’t exceed 512 MB
  * Max environment variables size can be 4KB
  * Compressed `.zip` and uncompressed code can’t exceed 50MB and 250MB respectively

## Application_Integration

Go to [Index](#index)

### SQS (Amazon Simple Queue Service)

![SQS](https://d1.awsstatic.com/product-page-diagram_Amazon-SQS%402x.8639596f10bfa6d7cdb2e83df728e789963dcc39.png)

* Fully managed, distributed Message Queue service that can be used for micro-services, distributed applications and serverless applications. In other words, a temporary repository for messages that are awaiting processing.
* It **decouples infraestructure** (acts like a buffer between) the software component producing/saving data and the component receiving data for processing.
* Specification for Standard SQS:
  * SQS guarantees that your messages will be processed at least once.
  * Can have unlimited number of messages waiting in queue
  * Default retention period is 4 days (min 1 min. and max 14 days)
  * Can send message upto 256KB in size (To send messages larger than 256 KB -up tp 2GB- using library allows you to send an Amazon SQS message that contains a reference to a message payload in Amazon S3)
  * Unlimited throughput and low latency (<10ms on publish and receive)
  * Can have duplicate messages (At least once delivery)
  * Can have out of order messages (best effort ordering)
* Consumer (can be EC2 instance or lambda function) **poll** the messages in batches (upto 10 messages) and delete them from queue after processing. If don’t delete, they stay in Queue and may process multiple times.
  * Polling types:
    * Short Polling (`ReceiveMessageWaitTimeSeconds` = 0) — Keeps polling queue looking for work, even if it’s empty.
    * Long Polling (`ReceiveMessageWaitTimeSeconds` > 0) - Reduces the number of empty responses by allowing Amazon SQS to wait until a message is available before sending a response to a ReceiveMessage request, helps to reduce the cost.
  * Visibility Timeout — the amount of time the message is invisible in the queue after reader picks it up. It prevents other consumers from receiving and processing the same message. The message is then deleted. If the job hasn’t completed it becomes visible in the queue again (can be max 12 hours).
    * Use Case: If you are getting messages delivered twice, the cause could be your visibility timeout is too low.

There are two types of queues: Standard & FIFO

#### Standard Queues

* Default queue type.
* Nearly unlimited number of API calls per second
* Guarantees message delivered at least once
* Occasionally more than one copy of a message might be delivered out of order. However, standard queues provide **best-effort ordering** which ensures that messages are generally delivered in the same order as they are sent

#### FIFO Queues

* First in First Out => The order in which the messages are sent is preserved.
* Has high throughput
* Limits: support up to 3,000 transactions per API batch call.
* Processed exactly once and duplicates are not introduced to the queue.

### SNS (Amazon Simple Notification Service)

* Fully managed Messaging Service that allows yo **push** (Instantaneous) messages on SNS topic and all topic subscribers receive those messages.
* Can group multiple recipients through topics.
* Highly available as all messages stored across multiple regions
* One topic can support deliveries to multiple endpoint types - for example, you can group together iOS, Android and SMS recipients, When you publish once to a topic, SNS delivers appropriately formatted copies of your message to each subscriber.
* Inexpensive, pay-as-you-go model with no up-front costs.

#### A2A (PubSub model)

* Allows for many-to-many messaging between distributed systems, microservices and other AWS Services
* Event driven.
* Decouple messages publishers from subscriber with a topic
* You can setup a Subscription Filter Policy which is JSON policy to send the filtered messages to specific subscribers.
* Subscribers can be Kinesis Data Firehose, SQS, HTTP, HTTPS, Lambda, Email, Email-JSON, SMS Messages, Mobile Notifications.

![A2A](https://d1.awsstatic.com/Product-Page-Diagram_Amazon-SNS_Event-Driven-SNS-Compute%402x.03cb54865e1c586c26ee73f9dff0dc079125e9dc.png)

#### A2P

* Lets you send messages to your **customers** with SMS texts, push notifications, and email.

![A2P_1](https://d1.awsstatic.com/Product-Page-Diagram_Amazon-SNS-SMS%402x.f499caaae8a9877fbefb4d9cf4768d030dc282da.png)

![A2P_2](https://d1.awsstatic.com/Product-Page-Diagram_Amazon-SNS-Mobile-Push%402x.08ac920f6c0bcf10c713be9e423b13e6fd9bd50c.png)

### Amazon MQ

* Amazon managed Apache ActiveMQ
* Migrate an existing message broker using MQTT protocol to AWS.

## Storage

Go to [Index](#index)

### S3 (Simple Storage Service)

![](https://d1.awsstatic.com/s3-pdp-redesign/product-page-diagram_Amazon-S3_HIW.cf4c2bd7aa02f1fe77be8aa120393993e08ac86d.png)

* Storage service that is highly scalable, secure and performant
* It is OBJECT BASED storage (suitable for files). It does not allow to install Operation System (different with EBS for example)
* S3 Object is made up of
  * Key →  Name of the object, full path of the object in bucket e.g. /movies/comedy/abc.avi
  * Value → data bytes of object (photos, videos, documents, etc.)
  * Version ID - version object (if versioning is enabled)
  * Metadata
  * Sub-resources (Access Control Lists & Torrent)
* S3 Bucket holds objects. S3 console show virtual folders based on key.
* There is unlimited storage, but individual files uploaded can be from **0 bytes to 5TB**.  You should use multi-part upload for Object size > 100MB
* S3 is a UNIVERSAL NAMESPACE, so bucket names need to be globally unique. The reason why is because it creates a web address (DNS name) with the buckets name in it
  * When you view Buckets you view them globally but you can have buckets in individual regions

```
https://<bucket-name>.s3.<aws-region>.amazonaws.com
or
https://s3.<aws-region>.amazonaws.com/<bucket-name>
```

* When you upload a file to S3, you receive a HTTP `200` code if the file upload is successful.
* **S3 Consistency**
  * Delivers **strong read-after-write consistency for PUTS and DELETES** of objects, for both new objects and for updates to existing objects. This means once there is a successful write, overwrite or delete — the next read request automatically receives the latest version of the object.
  * Updates to a single key are atomic. For example, if you PUT to an existing key from one thread and perform a GET on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data.
* In S3 you pay for the following things:
  * Storage
  * Requests and Data Retrievals
  * Storage Management Pricing
  * Data Transfer Pricing
  * Transfer Acceleration
  * Cross Region Replication Pricing
* We can change storage class and encryption on the fly

#### Optional features

* Enable `S3 Versioning` and `MFA` delete features to protect against accidental delete of S3 Object.
* Use `Object Lock` to store object using write-once-read-many (WORM) model to prevent objects from being deleted or overwritten for a fixed amount of time (`Retention period`) or indefinitely (`Legal hold`). Each version of object can have different retention-period.
* You can host static websites on S3 bucket consists of HTML, CSS, client-side JavaScript, and images. You need to enable Static website hosting and Public access for S3 to avoid 403 forbidden error. Also you need to add CORS Policy to allow cross origin request.

```
https://<bucket-name>.s3-website[.-]<aws-region>.amazonaws.com
```

**Note**: It is not used for dynamic websites or websites which require database, for ex: Wordpress...etc.

* `S3 Select` or `Glacier Select` can be used to retrieve subset of data from S3 Objects using SQL query. S3 Objects can be CSV, JSON, or Apache Parquet. GZIP & BZIP2 compression is supported with CSV or JSON format with server-side encryption.
  * Allows you to save money on data transfer and increase speed.
* Using `Range` HTTP Header in a GET Request to download the specific range of bytes of S3 object, known as Byte Range Fetch
* You can create `S3 event notification` to push events e.g. s3:ObjectCreated:* to SNS topic, SQS queue or execute a Lambda function. It is possible that you receive single notification for two writes to non-versioned object at the same time. Enable versioning to ensure you get all notifications.
* Enable `S3 Cross-Region Replication` for asynchronous replication of object across buckets in another region.
  * Cross Region Replication REQUIRES versioning to be ENABLED on both SOURCE & DESTINATION bucket.
  * Files in an existing bucket are not replicated automatically once this is enabled — only subsequent updated files. A new objects
  * You can have this enabled for the entire bucket or just for specific prefixes
  * Delete markers ARE NOT replicated
* Enable `Server access logging` for logging object-level fields object-size, total time, turn around time, and Http referrer. Not available with CloudTrail.
* Use `VPC S3 gateway endpoint` to access S3 bucket within AWS VPC to reduce the overall data transfer cost.
* Enable `S3 Transfer Acceleration` for faster transfer and high throughput to S3 bucket (mainly uploads)
  * Create CloudFront distribution with Origin Access Identity (OAI) pointing to S3 for faster cached content delivery (mainly reads) over long distances between your client and S3.
  * Restrict the access of S3 bucket through CloudFront only using Origin Access Identity (OAI). Make sure user can’t use a direct URL to the S3 bucket to access the file.
* Use AWS Athena (Serverless Query Engine) to perform analytics directly against S3 objects using SQL query and save the analysis report in another S3 bucket.
  * Use Case: one time SQL query on S3 objects, S3 access log analysis, serverless queries on S3, IoT data analytics in S3, etc.
* Locks
  * Use `Object Lock` to store object using Write-Once-Read-Many (WORM) model to prevent objects from being deleted or overwritten for a custom-defined retention period or indefinitely. It can be one bucket or individual elements.
    * You can use S3 Object lock to meet regulatory requirements that require WORM storage, or add an extra layer of protection against object changes and deletion.
    * Amazon S3 currently does not support enabling object lock after a bucket has been created.
    * S3 has two types of retention mode:
      * Governance Mode → Users can’t overwrite , delete or alter the object version locked unless they have special permissions (permissions requires to be granted).
      * Compliance Mode → A protected object version can’t be overwritten or deleted by ANY user including the root user during its retention period
    * `Retention Period` → period that protects an object version for a fixed amount of time. Once it expires the object can be overwritten. Unless there is a LEGAL HOLD placed on its version.
    * `Legal Hold` → Prevents object version from being overwritten or deleted. It doesn’t have a retention period, it remains in effect until it is removed.
  * `Glacier Vault Lock` → enforce compliance controls on individual S3 Glacier vaults using a Vault Lock policy.

#### S3 Tiered Storage (Storage Classes)

* You can upload files in the same bucket with different Storage Classes like S3 standard, Standard-IA, One Zone-IA, Glacier etc.
* You can setup `S3 Lifecycle Rules` to transition current (or previous version) objects to cheaper storage classes or delete (expire if versioned) objects after certain days for e.g.
  * Use Case:
    * Transition from S3 Standard to S3 Standard-IA or One Zone-IA can only be done after 30 days.
  * You can also setup lifecycle rule to abort multipart upload, if it doesn’t complete within certain days, which auto delete the parts from S3 buckets associated with multipart upload.
  * It can be u
* Princing per Storage type
  * S3 Glacier Deep Archive is the cheapest.
  * S3 Standard is the most expensive, if you are going to use it — why not use S3 Intelligent tiering (same price), unless you have thousands or millions of objects.
    * Benefit of using  S3 Intelligent tiering: it does give you access to the infrequently access — so you could save money!
    * Warning: If you have a lot of objects you are going to incur monitoring and automation charges.

| S3 Storage Class | Durability | Availability |  AZ |  Min. Storage | Retrieval Time | Retrieval fee|
|---|---|---|---|---|---|---|
S3 Standard (General Purpose) | 11 9’s | 99.99% | ≥3 | N/A | milliseconds | N/A |
S3 Intelligent Tiering | 11 9’s | 99.9% | ≥3 | 30 days | millisecond | N/A
S3 Standard-IA (Infrequent Access) | 11 9’s | 99.9% | ≥3 | 30 days | milliseconds | per GB
S3 One Zone-IA (Infrequent Access) |11 9’s |99.5% | 1 | 30 days | milliseconds |per GB
S3 Glacier | 11 9’s | 99.99% | ≥3 | 90 days | Expedite (1-5 mins), Standard (3-5 hrs), Bulk (5-12 hrs) | per GB
S3 Glacier Deep Archive | 11 9’s | 99.99% | ≥3 | 180 days | Standard (12 hrs), Bulk (48 hrs)  per GB

* `Standard`: General purpose storage for any type of frequently used data very high availability, and fast retrieval
  * HA: Stored redundantly across multiple devices in multiple facilities and is designed to sustain the loss of 2 facilities concurrently.
* `Intelligent Tiering`: Analyze your Object’s usage and move them to the appropriate cost-effective storage class automatically, without performance impact or operational overhead.
  * Use case: automatic cost savings for data with unknown or changing access patterns
* `Standard-IA` (Infrequently Accessed) : Cost effective for infrequent access files which cannot be recreated
  * For data that is not accessed very frequently — but once it is accessed it needs to be retrieved rapidly.
  * Is cheaper than standard S3, but you do get charged a retrieval fee
* `One-Zone IA`(also called S3 RRS) : Cost effective for infrequent access files which can be recreated
  * Low cost option for data that is not accessed frequently and does not require the redundancy, if the zone fails, we loose the data.
  * Use case: re-creatable infrequently accessed data that needs milliseconds access.
* `Glacier`: Cheaper choice to Archive Data. Retrival time configurable from minutes to hours
* `Glacier Deep Archive`: Cheapest choice for Long-term storage of large amount of data for compliance. Retrival time configurable but slower than `Glacier`, strating from 12 hours.

#### Sharing S3 buckets Across Accounts

If you have two accounts within the same organisation you can use any of these to share the an S3 bucket with both accounts:

* Bucket policy & IAM — applies to entire bucket, but programmatic access only
* Using bucket ALCs & IAM — can apply to individual objects — programatic access only
* Cross Account IAM roles — programatic and console access

#### S3 Security

* By default newly created buckets are private, but you can make them public if needed, for example - you would need to make it public for static web hosting purposes.

#####  Access

###### Access Control lists (deprecated)

* Access Control Lists can be for **individual files**. Can grant basic read and write permissions at an object level (not just whole bucket)
* For example: use if there is a file in a bucket you don’t want everyone to have access to.

###### Bucket policy (recommended)

* S3 Bucket Policies are JSON based policy for complex access rules at user, account, folder, and object level
* Bucket policies are **bucket wide**. This works at budget levels not individual file level. Applies to whole bucket!

###### S3 Signed URLS

* Used to secure content so that only people you authorise are able to access (upload or download object data) temporarly it.
* It can be generated from CLI or SDK (can’t from web) and has an LIMITED LIFETIME (e.g. 5 min)

```
aws s3 presign s3://mybucket/myobject --expires-in 300
```

* Use Case: when not using CloudFront (Different from CloudFront signed urls) and user have direct access to S3
* Issues a request as the IAM user who creates the pre-signed URL (Same permissions)

##### Encryption

* `Encryption in Transit` — encrypting network traffic (between client and S3) using SSL/TLS
* `Encryption at Rest (Server Side)` — happens server side, encrypting the data which is stored. Can be achieved by:
  * `SSE-S3`: S3 Managed Keys (SSE-S3), AWS Managed Keys
  * `SSE-KMS`: AWS Key Management Service(SSE-KMS) AWS & you manage keys together
  * `SSE-C`: Customer provided keys — give AWS you own keys that you manage.
* `Encryption at Rest (Client Side)` — client encrypt and decrypt the data before sending and after receiving data from S3
* To meet PCI-DSS or HIPAA compliance, encrypt S3 using SSE-C and Client Side Encryption

#### S3 Versioning

* It acts like a backup tool that stores all versions of an object (even writes & deletes)
  * If you delete a file it will still show up in versioning with the delete marker on it.
* When enabled on your bucket it cannot be disabled — only suspended
* It is possible to integrate it with life cycle rules
* If you mark a single file as public and then upload a new version of it — the new version is private
* The size of your S3 bucket is the sum of all files and all versions of those files
* Versioning's MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security.

#### S3 Performance

S3 Has extremely low latency

##### Performance limitations

* If you are using KMS (SSE-KMS) to encrypt your objects in S3, you must keep in mind the KMS limits.
  * When you upload a file, you will call GenerateDataKey in the KMS API.
  * When you download a file, you will call Decrypt in the KMS API.
* Uploading/Downloading will count towards the KMS per second quota, which could affect performance
  * Region-specific, however, it's either 5,500, 10,000 or 30,000 requests our second.
  * accessed through a Network File System (NFS) mount point

##### Improving Performance

* `S3 Prefix` is the part between the bucket name and the filename. You can get better performance by spreading your reads across different prefixes.

```
mybucketname/folder1/subfolder1/myfile.jpg >  /folder1/subfolder1 is the prefix
```

Use Case: By default, you can get the first byte out of S3 within 100-200 milliseconds. You can also achieve a high number of requests: 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix. Spreading your reads across different prefixes. For Example, If you are using two prefixes, you can achieve 11,000 requests per second.

* `Multipart Uploads` -> it splits your file into parts and uploads them in Parallel
  * Recommended for files over 100MB
  * Required for files over 5GB

* For download this is call `S3 Byte Range Fetches` — Parallelises download by specifying byte ranges, which speeds up downloads and can download partial amounts of info.

#### S3 Storage Gateway

* Is a hybrid cloud storage service for connecting on-premises software applications with cloud based storage.
* Allows your on-premise to access virtually unlimited cloud storage.
* Can be downloaded as a Virtual Machine Image and installed in your datacenter.
* Has low latency as it caches data in the local VM or gateway hardware appliance.
* Storage Gateways Type
  * **1.** File Gateway (protocol NFS & SMB)
    * Stores objects directly in s3
    * Utilises standard storage protocols with NFS & SMB
    * Common use case is for on-premise backup to the cloud
  * **2.** Volume Gateway (ISCSI block protocol)
    * Presents your applications with disk volumes using ISCSI block protocol
    * Stores/manages on-premise data in S3
    * It allows you to take point-in-time snapshots using AWS Backup and stores them in EBS (Only captures changed blocks)
    * Types of Volume Gateways:
      * Stored Volumes — Entire Dataset is stored on site and is asynchronously backed up to S3. Store you primary data locally so there is low latency to the entire dataset and then asynchronously backs up that data to S3.
      * Cached Volumes — Entire Dataset is stored on S3 and the most frequently accessed data is cached on site. Uses s3 as your primary storage while retaining frequently accessed data locally. Minimise need to scale your on-premise infrastructure
  * **3.** Tape Gateway (VTL)
    * Durable, cost effective archiving
    * Is a way of replacing physical tapes with a virtual tape interface in AWS without changes existing backup workflows

###  Instance Store

* Instance Store is temporary block-based storage physically attached to an EC2 instance
* It can be attached to an EC2 instance only when the instance is launched and cannot be dynamically resized
* Also known to as Ephemeral Storage
* Deliver very low-latency and high random I/O performance
* Data persists on instance reboot, data doesn’t persist on stop or termination

### EBS (Elastic Block Store)

* EBS is persistent storage volumes for EC2
* It is Block-based storage: It needs to be **mounted to an EC2 instance within the same Availability Zone** (EBS Volume think like a "USB stick")
  * 1 EBS - 1 EC2. It can be attached to only one EC2 instance at a time. Can be detached & attached to another EC2 instance in that same AZ only.
  * 1 EC2 - 1..N EBS. Can attach multiple EBS volumes to single EC2 instance. Data persist after detaching from EC2
* EBS Snapshots
  * It is a backup of EBS Volume at a point in time.
  * You can not copy EBS volume across AZ but you can **create EBS Volume from Snapshot across AZ**, including different AWS Regions.
* EBS supports dynamic changes in live production volume e.g. volume type, volume size, and IOPS capacity without service interruption
* EBS Volume encryption:
  * All data at rest inside the volume is encrypted
  * All data in flight between the volume and EC2 instance is encrypted
  * All snapshots of encrypted volumes are automatically encrypted
  * All volumes created from encrypted snapshots are automatically encrypted
  * Volumes created from unencrypted snapshots can be encrypted at the time of creation
* Types of EBS volumes:
  * SSD for small/random IO operations, High IOPS means number of read and write operations per second, Only SSD EBS Volumes can be used as boot volumes for EC2
  * HDD for large/sequential IO operations, High Throughput means number of bytes read and write per second
* EBS Volumes with two types of RAID configuration:-
  * RAID 0 (increase performance) two 500GB EBS Volumes with 4000 IOPS - creates 1000GB RAID0 Array with 8000 IOPS and 1000Mbps throughput
  * RAID 1 (increase fault tolerance) two 500GB EBS Volumes with 4000 IOPS - creates 500GB RAID1 Array with 4000 IOPS and 500Mbps throughput

EBS VolumeTypes | Description | Usage |
|---|---|---|
General Purpose SSD (gp2/gp3) | Max 16000 IOPS | boot volumes, dev environment, virtual desktop|
Provisioned IOPS SSD (io1/io2)| 16000 - 64000 IOPS, EBS Multi-Attach | critical business application, large SQL and NoSQL database workloads |
Throughput Optimized HDD (st1) | Low-cost, frequently accessed, throughput intensive | Big Data, Data warehouses, log processing |
Cold HDD (sc1) | Lowest-cost, infrequently accessed | Large data with lowest cost|

### EFS (Elastic File System)

* EFS is a POSIX-compliant file-based storage
* EFS supports file systems semantics - strong read after write consistency and file locking
* highly scalable - can automatically scale from gigabytes to petabytes of data without needing to provision storage. With burst mode, the throughput increase, as file system grows in size.
* Highly Available - stores data redundantly across multiple Availability Zones
* Network File System (NFS) that can be mounted on and accessed concurrently by thousands of EC2 in multiple AZs without sacrificing performance.
* EFS file systems can be accessed by Amazon EC2 Linux instances, Amazon ECS, Amazon EKS, AWS Fargate, and AWS Lambda functions via a file system interface such as NFS protocol.
* Performance Mode:
  * General Purpose for most file system for low-latency file operations, good for content-management, web-serving etc.
  * Max I/O is optimized to use with 10s, 100s, 1000s of EC2 instances with high aggregated throughput and IOPS, slightly higher latency for file operations, good for big data analytics, media processing workflow
* Use case: Share files, images, software updates, or computing across all EC2 instances in ECS, EKS cluster

### FSx for Windows

* Windows-based file system supports SMB protocol & Windows NTFS
* Supports Microsoft Active Directory (AD) integration, ACLs, user quotas

### FSx for Lustre

* Lustre = Linux + Cluster is a **POSIX-compliant parallel linux file system**, which stores data across multiple network file servers
* High performance file system for **fast processing of workload** with consistent **sub-millisecond latencies**, up to hundreds of gigabytes per second of throughput, and up to millions of IOPS.
* Use it for Machine learning, High performance computing (HPC), video processing, financial modeling, genome sequencing, and electronic design automation (EDA).
* You can use **FSx for Lustre as hot storage** for your highly accessed files, and **Amazon S3 as cold storage** for rarely accessed files.
* **Seamless integration with Amazon S3** - connect your S3 data sets to your FSx for Lustre file system, run your analyses, write results back to S3, and delete your file system
* FSx for Lustre provide two deployment options:
  * **Scratch file systems** - for temporary storage and short term processing
  * **Persistent file systems** - for high available & persist storage and long term processing

## References

After doing the course from [Digital Training](https://digitalcloud.training/aws-cheat-sheets/), I reviwed the following resources to make this summary:

* [Amazon Docs](https://aws.amazon.com/)
* [AWS Certified Solutions Architect Associate (SAA-C02) Exam Notes - Coding N Concepts](https://codingnconcepts.com/aws/aws-certified-solutions-architect-associate/#aws-infrastructure)
* [AWS\_CCP\_Notes/AWS\_Solution\_Architecture\_Associate.txt at main · kasukur/AWS\_CCP\_Notes](https://github.com/kasukur/AWS_CCP_Notes/blob/main/AWS_Solution_Architecture_Associate.txt)
* [AWS Solution Architect Associate Exam Study Notes | by Chloe McAteer | Medium](https://chloemcateer.medium.com/aws-solution-architect-associate-exam-study-notes-b6c5884ee500)

Finally, I practiced the following Exam Tests
